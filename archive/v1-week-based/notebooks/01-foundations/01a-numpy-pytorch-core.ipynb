{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01a: NumPy & PyTorch Primer — Core\n",
    "\n",
    "**Week 1, Days 1-2** | Foundations\n",
    "\n",
    "**Prerequisites**: Basic Python (you're a systems programmer, so ✓)\n",
    "\n",
    "**Time**: ~60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Create and manipulate tensors in NumPy and PyTorch\n",
    "- [ ] Use broadcasting to operate on arrays of different shapes\n",
    "- [ ] Apply advanced indexing to slice and filter data\n",
    "- [ ] Compute gradients automatically with autograd\n",
    "- [ ] Move tensors between CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Check what we're working with\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS (Apple Silicon) available: True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# INTRO\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "![Code becoming vectors](hero-intro.png)\n",
    "\n",
    "## The Setup\n",
    "\n",
    "You're building a code search tool. Not grep—*semantic* search. The kind that knows `allocate_buffer()` and `malloc()` are related even though they share zero characters.\n",
    "\n",
    "The trick? Turn code into vectors. Functions that do similar things end up as similar vectors. Search becomes \"find the vectors closest to my query.\"\n",
    "\n",
    "Your prototype needs to:\n",
    "1. Store code embeddings (vectors, lots of them)\n",
    "2. Compute similarity between query and all snippets (fast matrix math)\n",
    "3. Extract the top results (slicing and filtering)\n",
    "4. Eventually: learn better embeddings from user feedback (gradients)\n",
    "5. Eventually: not take forever (GPU)\n",
    "\n",
    "Today we're building the foundation. By the end of this notebook, you'll have a working similarity search over synthetic embeddings—all the tensor operations you need for the real thing.\n",
    "\n",
    "Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 0: THE PROBLEM\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## What Are We Actually Building?\n",
    "\n",
    "Semantic code search works like this:\n",
    "\n",
    "```\n",
    "1. Encode all code snippets as vectors (embeddings)\n",
    "2. Encode the search query as a vector\n",
    "3. Find code vectors closest to query vector\n",
    "4. Return those snippets\n",
    "```\n",
    "\n",
    "The \"encoding\" part is a neural network (we'll get there in Week 2). Today's job is everything else: storing vectors, computing distances, extracting results.\n",
    "\n",
    "Here's what the data looks like:\n",
    "\n",
    "- **Code corpus**: 1000 functions, each embedded as a 384-dimensional vector\n",
    "- **Query**: A single 384-dimensional vector\n",
    "- **Similarity**: Cosine similarity (dot product of normalized vectors)\n",
    "- **Output**: Top 10 most similar functions\n",
    "\n",
    "Simple enough, right? Let's see what tools we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 1: INTUITION — Tensors\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## What *is* a tensor?\n",
    "\n",
    "Imagine organizing data in space:\n",
    "\n",
    "- **A single number** (similarity score: 0.87) is a **scalar** — a point\n",
    "- **A list of numbers** (one embedding: [0.1, -0.3, 0.8, ...]) is a **vector** — a line\n",
    "- **A grid of numbers** (all 1000 embeddings stacked) is a **matrix** — a sheet\n",
    "- **A cube** (batch of users × batch of queries × embedding dims) is a **3D tensor**\n",
    "\n",
    "And it keeps going. A **tensor** is just an n-dimensional array of numbers.\n",
    "\n",
    "```\n",
    "0D: scalar       → shape: ()        → similarity score\n",
    "1D: vector       → shape: (384,)    → one embedding\n",
    "2D: matrix       → shape: (1000, 384)   → all embeddings\n",
    "3D: 3-tensor     → shape: (32, 1000, 384)   → batched search\n",
    "```\n",
    "\n",
    "The **rank** of a tensor is how many indices you need to grab a single number. A matrix needs two (row, column). A 3D tensor needs three.\n",
    "\n",
    "For our code search:\n",
    "- Query embedding: 1D tensor, shape (384,)\n",
    "- Corpus embeddings: 2D tensor, shape (1000, 384)\n",
    "- Similarity scores: 1D tensor, shape (1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 2: CODE + VIZ — Tensors\n",
    "# ═══════════════════════════════════════════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 1: Create the Corpus Embeddings\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Let's start by creating synthetic embeddings for our code corpus. In the real system, these would come from a neural network. For now, we'll use random vectors (normalized, so they live on the unit sphere).\n",
    "\n",
    "Your task:\n",
    "1. Create a `corpus` tensor of shape (1000, 384) with random values\n",
    "2. Normalize each row to have unit length (L2 norm = 1)\n",
    "3. Verify the shape and that rows are normalized\n",
    "\n",
    "Hint: To normalize, divide each row by its L2 norm. Use `np.linalg.norm(x, axis=1, keepdims=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_functions = 1000\n",
    "embedding_dim = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "# Create random embeddings\n",
    "corpus_raw = None  # TODO: shape (1000, 384)\n",
    "\n",
    "# Normalize each row\n",
    "corpus = None  # TODO: divide by row norms\n",
    "\n",
    "# Verify\n",
    "print(f\"Corpus shape: {corpus.shape}\")\n",
    "print(f\"First row norm: {np.linalg.norm(corpus[0]):.4f}\")\n",
    "print(f\"Last row norm: {np.linalg.norm(corpus[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ corpus_raw = np.random.randn(n_functions, embedding_dim)\n",
    "# │ norms = np.linalg.norm(corpus_raw, axis=1, keepdims=True)\n",
    "# │ corpus = corpus_raw / norms\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "assert corpus.shape == (1000, 384), f\"Expected shape (1000, 384), got {corpus.shape}\"\n",
    "assert np.allclose(np.linalg.norm(corpus, axis=1), 1.0), \"Rows should have unit norm\"\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 2: Create the Query Embedding\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Now let's create a query embedding. In the real system, this would be the embedding of the user's search query (like \"memory allocation function\"). We'll simulate one.\n",
    "\n",
    "Your task:\n",
    "1. Create a `query` tensor of shape (384,) with random values\n",
    "2. Normalize it to unit length\n",
    "3. Verify the shape and norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "# Create random query embedding\n",
    "query_raw = None  # TODO: shape (384,)\n",
    "\n",
    "# Normalize\n",
    "query = None  # TODO\n",
    "\n",
    "# Verify\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Query norm: {np.linalg.norm(query):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ query_raw = np.random.randn(embedding_dim)\n",
    "# │ query = query_raw / np.linalg.norm(query_raw)\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "assert query.shape == (384,), f\"Expected shape (384,), got {query.shape}\"\n",
    "assert np.isclose(np.linalg.norm(query), 1.0), \"Query should have unit norm\"\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what we've built. High-dimensional vectors are hard to picture, so we'll project down to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "# Visualize using PCA projection to 2D (just for intuition)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "corpus_2d = pca.fit_transform(corpus)\n",
    "query_2d = pca.transform(query.reshape(1, -1))[0]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(corpus_2d[:, 0], corpus_2d[:, 1], alpha=0.3, s=10, label='Code snippets')\n",
    "plt.scatter(query_2d[0], query_2d[1], color='red', s=200, marker='*', label='Query', zorder=5)\n",
    "plt.xlabel('PCA dimension 1')\n",
    "plt.ylabel('PCA dimension 2')\n",
    "plt.title('Code Embeddings (projected to 2D)\\nRed star = search query')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 1: INTUITION — Broadcasting\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## Computing similarity: the naive way vs the smart way\n",
    "\n",
    "To find similar code, we need to compute the cosine similarity between our query and *every* snippet in the corpus. Cosine similarity between normalized vectors is just the dot product.\n",
    "\n",
    "The naive way:\n",
    "```python\n",
    "similarities = []\n",
    "for i in range(1000):\n",
    "    sim = np.dot(query, corpus[i])\n",
    "    similarities.append(sim)\n",
    "```\n",
    "\n",
    "This works, but it's slow. We're calling `np.dot` 1000 times, and Python loops are molasses.\n",
    "\n",
    "The smart way: **Broadcasting**.\n",
    "\n",
    "Broadcasting lets NumPy/PyTorch \"stretch\" arrays to compatible shapes and compute element-wise operations in C (fast). When we write `corpus @ query`, we get all 1000 dot products in one operation.\n",
    "\n",
    "```\n",
    "corpus: (1000, 384)\n",
    "query:        (384,)\n",
    "─────────────────────\n",
    "result: (1000,)       ← each row dotted with query\n",
    "```\n",
    "\n",
    "The shapes align from the right. The `@` operator does matrix-vector multiplication, computing 1000 dot products at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 2: CODE + VIZ — Broadcasting\n",
    "# ═══════════════════════════════════════════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 3: Compute Similarity Scores\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Now let's compute the similarity between our query and every code snippet in the corpus. Since both are normalized, cosine similarity = dot product.\n",
    "\n",
    "Your task:\n",
    "1. Compute `similarities` using matrix-vector multiplication: `corpus @ query`\n",
    "2. Verify the shape is (1000,)\n",
    "3. Find the range of similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "# Compute all similarities at once\n",
    "similarities = None  # TODO: corpus @ query\n",
    "\n",
    "# Verify\n",
    "print(f\"Similarities shape: {similarities.shape}\")\n",
    "print(f\"Min similarity: {similarities.min():.4f}\")\n",
    "print(f\"Max similarity: {similarities.max():.4f}\")\n",
    "print(f\"Mean similarity: {similarities.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ similarities = corpus @ query\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "assert similarities.shape == (1000,), f\"Expected shape (1000,), got {similarities.shape}\"\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "# Visualize the similarity distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(similarities, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(similarities.mean(), color='red', linestyle='--', label=f'Mean: {similarities.mean():.3f}')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Similarity Scores\\n(Query vs All Code Snippets)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity scores form a bell curve around 0. This makes sense—random vectors in high dimensions are nearly orthogonal (uncorrelated). The ones with high scores happened to point in a similar direction by chance.\n",
    "\n",
    "With real embeddings, similar code would cluster, creating meaningful high-similarity matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 1: INTUITION — Indexing\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## Extracting the results we care about\n",
    "\n",
    "We have 1000 similarity scores. But users don't want all 1000—they want the top 10 most similar snippets.\n",
    "\n",
    "NumPy's indexing is incredibly powerful. You can:\n",
    "- Grab specific elements: `arr[5]`\n",
    "- Grab slices: `arr[10:20]`\n",
    "- Grab by condition: `arr[arr > 0.1]`\n",
    "- Grab by index array: `arr[[3, 7, 12, 99]]`\n",
    "\n",
    "For our search, we need to:\n",
    "1. Find the *indices* of the top 10 scores: `np.argsort()` + slicing\n",
    "2. Extract those scores: fancy indexing with the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 2: CODE + VIZ — Indexing\n",
    "# ═══════════════════════════════════════════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 4: Find Top-K Results\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Now let's extract the top 10 search results. We need the indices (which snippets?) and the scores (how similar?).\n",
    "\n",
    "Your task:\n",
    "1. Use `np.argsort(similarities)` to get indices that would sort by similarity\n",
    "2. Take the last 10 indices (highest scores) and reverse them (descending order)\n",
    "3. Use those indices to get the corresponding similarity scores\n",
    "\n",
    "Hint: `argsort` returns indices in ascending order. `[-10:][::-1]` gets the last 10, reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "k = 10  # Number of results to return\n",
    "\n",
    "# Get indices that would sort similarities (ascending)\n",
    "sorted_indices = None  # TODO: np.argsort(...)\n",
    "\n",
    "# Get top-k indices (highest scores, descending)\n",
    "top_k_indices = None  # TODO: last k, reversed\n",
    "\n",
    "# Get corresponding scores\n",
    "top_k_scores = None  # TODO: fancy indexing\n",
    "\n",
    "# Display results\n",
    "print(\"Top 10 Search Results:\")\n",
    "print(f\"{'Rank':<6} {'Index':<10} {'Similarity':<12}\")\n",
    "print(\"-\" * 28)\n",
    "for rank, (idx, score) in enumerate(zip(top_k_indices, top_k_scores), 1):\n",
    "    print(f\"{rank:<6} {idx:<10} {score:<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ sorted_indices = np.argsort(similarities)\n",
    "# │ top_k_indices = sorted_indices[-k:][::-1]\n",
    "# │ top_k_scores = similarities[top_k_indices]\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "assert len(top_k_indices) == 10, f\"Expected 10 indices, got {len(top_k_indices)}\"\n",
    "assert top_k_scores[0] >= top_k_scores[-1], \"Scores should be in descending order\"\n",
    "assert top_k_scores[0] == similarities.max(), \"First score should be the maximum\"\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 5: Threshold Filtering\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Sometimes we want to filter by a minimum similarity threshold instead of (or in addition to) top-k. Let's find all snippets with similarity > 0.15.\n",
    "\n",
    "Your task:\n",
    "1. Create a boolean mask: `similarities > 0.15`\n",
    "2. Use the mask to get matching indices: `np.where(mask)[0]`\n",
    "3. Count how many matches we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "threshold = 0.15\n",
    "\n",
    "# Create boolean mask\n",
    "above_threshold = None  # TODO: similarities > threshold\n",
    "\n",
    "# Get indices of True values\n",
    "matching_indices = None  # TODO: np.where(...)\n",
    "\n",
    "# Get matching scores  \n",
    "matching_scores = None  # TODO: fancy indexing\n",
    "\n",
    "print(f\"Snippets above threshold ({threshold}): {len(matching_indices)}\")\n",
    "print(f\"Scores: {matching_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ above_threshold = similarities > threshold\n",
    "# │ matching_indices = np.where(above_threshold)[0]\n",
    "# │ matching_scores = similarities[matching_indices]\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "assert all(similarities[matching_indices] > threshold), \"All matches should be above threshold\"\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "# Visualize: highlight top results in 2D projection\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(corpus_2d[:, 0], corpus_2d[:, 1], alpha=0.3, s=10, c='gray', label='All snippets')\n",
    "plt.scatter(corpus_2d[top_k_indices, 0], corpus_2d[top_k_indices, 1], \n",
    "            c='blue', s=100, marker='o', label='Top 10 results', zorder=4)\n",
    "plt.scatter(query_2d[0], query_2d[1], color='red', s=200, marker='*', label='Query', zorder=5)\n",
    "\n",
    "# Add rank labels\n",
    "for rank, idx in enumerate(top_k_indices[:5], 1):\n",
    "    plt.annotate(f'#{rank}', (corpus_2d[idx, 0], corpus_2d[idx, 1]), fontsize=10)\n",
    "\n",
    "plt.xlabel('PCA dimension 1')\n",
    "plt.ylabel('PCA dimension 2')\n",
    "plt.title('Search Results Highlighted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 1: INTUITION — Autograd\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## Making the embeddings better\n",
    "\n",
    "So far we've been using random embeddings. They work, but they're not meaningful. Real semantic search needs embeddings where similar code → similar vectors.\n",
    "\n",
    "How do we learn better embeddings? **Gradients**.\n",
    "\n",
    "Here's the idea:\n",
    "1. Define what \"good\" means (a loss function)\n",
    "2. Compute how to tweak each embedding to make the loss smaller (gradients)\n",
    "3. Tweak and repeat\n",
    "\n",
    "Computing gradients by hand is tedious and error-prone. **Autograd** does it automatically:\n",
    "1. You write the forward computation (query → similarity → loss)\n",
    "2. PyTorch records every operation\n",
    "3. Call `loss.backward()` and PyTorch replays backwards, computing gradients via chain rule\n",
    "\n",
    "Think of it like a tape recorder. Forward pass: record. Backward pass: replay in reverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 2: CODE + VIZ — Autograd\n",
    "# ═══════════════════════════════════════════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 6: Convert to PyTorch and Track Gradients\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Let's switch to PyTorch and make our embeddings learnable. We'll convert the corpus and query to PyTorch tensors, and tell PyTorch to track gradients for the corpus embeddings.\n",
    "\n",
    "Your task:\n",
    "1. Convert `corpus` (NumPy) to `corpus_pt` (PyTorch) with `requires_grad=True`\n",
    "2. Convert `query` to `query_pt` (no gradients needed for query)\n",
    "3. Verify both are PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "corpus_pt = None  # TODO: torch.tensor(..., requires_grad=True, dtype=torch.float32)\n",
    "query_pt = None   # TODO: torch.tensor(..., dtype=torch.float32)\n",
    "\n",
    "# Verify\n",
    "print(f\"corpus_pt type: {type(corpus_pt)}\")\n",
    "print(f\"corpus_pt requires_grad: {corpus_pt.requires_grad}\")\n",
    "print(f\"query_pt requires_grad: {query_pt.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ corpus_pt = torch.tensor(corpus, requires_grad=True, dtype=torch.float32)\n",
    "# │ query_pt = torch.tensor(query, dtype=torch.float32)\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "assert isinstance(corpus_pt, torch.Tensor), \"corpus_pt should be a PyTorch tensor\"\n",
    "assert corpus_pt.requires_grad, \"corpus_pt should have requires_grad=True\"\n",
    "assert not query_pt.requires_grad, \"query_pt should not require gradients\"\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 7: Compute a Loss and Backpropagate\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Let's define a simple learning objective: we want snippet #42 to be the most similar to the query. (In reality, this signal would come from user clicks.)\n",
    "\n",
    "Our loss will be: `-similarity[42]` (negative because we want to maximize similarity, but optimization minimizes loss)\n",
    "\n",
    "Your task:\n",
    "1. Compute similarities: `corpus_pt @ query_pt`\n",
    "2. Define loss: negative of the similarity for snippet #42\n",
    "3. Call `loss.backward()` to compute gradients\n",
    "4. Print the gradient shape and a sample of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "target_idx = 42  # We want this snippet to be most similar\n",
    "\n",
    "# Compute similarities\n",
    "sims_pt = None  # TODO: matrix-vector product\n",
    "\n",
    "# Define loss (we want to MAXIMIZE similarity, so MINIMIZE negative similarity)\n",
    "loss = None  # TODO: -sims_pt[target_idx]\n",
    "\n",
    "# Backpropagate\n",
    "# TODO: loss.backward()\n",
    "\n",
    "# Inspect gradients\n",
    "print(f\"Loss value: {loss.item():.4f}\")\n",
    "print(f\"Gradient shape: {corpus_pt.grad.shape}\")\n",
    "print(f\"Gradient for snippet 42 (first 10 dims): {corpus_pt.grad[42, :10]}\")\n",
    "print(f\"Gradient for snippet 0 (first 10 dims): {corpus_pt.grad[0, :10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ sims_pt = corpus_pt @ query_pt\n",
    "# │ loss = -sims_pt[target_idx]\n",
    "# │ loss.backward()\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "assert corpus_pt.grad is not None, \"Gradients should have been computed\"\n",
    "assert corpus_pt.grad.shape == (1000, 384), \"Gradient shape should match corpus\"\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something interesting:\n",
    "- Snippet 42's gradient is non-zero: `-query` (the direction that increases its similarity)\n",
    "- Other snippets' gradients are zero (they don't affect the loss)\n",
    "\n",
    "If we used a more complex loss (like contrastive loss over all snippets), all gradients would be non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 1: INTUITION — GPU\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## Scaling up\n",
    "\n",
    "Our corpus has 1000 embeddings. A real codebase might have millions. The GPU is how we make this fast.\n",
    "\n",
    "Your CPU is like a brilliant professor: one thing at a time, very well.\n",
    "\n",
    "Your GPU is like a classroom of 1000 students: each can only do simple math, but they all work simultaneously.\n",
    "\n",
    "Matrix multiplication (which is what `corpus @ query` does) is \"embarrassingly parallel\"—each output element can be computed independently. Perfect for the GPU.\n",
    "\n",
    "Moving data to the GPU is easy: `.to(device)` or `.cuda()` / `.mps()`. The catch: all tensors in an operation must be on the same device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LAYER 2: CODE + VIZ — GPU\n",
    "# ═══════════════════════════════════════════════════════════════════════════════"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 8: Move to GPU and Benchmark\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Let's move our tensors to the GPU (if available) and compare performance.\n",
    "\n",
    "Your task:\n",
    "1. Detect the best available device (cuda, mps, or cpu)\n",
    "2. Move corpus and query to that device\n",
    "3. Run a timing comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "# Detect best device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move tensors to device (create fresh tensors without gradient history)\n",
    "corpus_gpu = None  # TODO: torch.tensor(corpus, device=device, dtype=torch.float32)\n",
    "query_gpu = None   # TODO: torch.tensor(query, device=device, dtype=torch.float32)\n",
    "\n",
    "# Verify\n",
    "print(f\"corpus_gpu device: {corpus_gpu.device}\")\n",
    "print(f\"query_gpu device: {query_gpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ corpus_gpu = torch.tensor(corpus, device=device, dtype=torch.float32)\n",
    "# │ query_gpu = torch.tensor(query, device=device, dtype=torch.float32)\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "# Benchmark: CPU vs GPU (or whatever we have)\n",
    "import time\n",
    "\n",
    "# Larger corpus for meaningful benchmark\n",
    "large_corpus_np = np.random.randn(100000, 384).astype(np.float32)\n",
    "large_corpus_np = large_corpus_np / np.linalg.norm(large_corpus_np, axis=1, keepdims=True)\n",
    "\n",
    "# CPU timing\n",
    "large_corpus_cpu = torch.tensor(large_corpus_np)\n",
    "query_cpu = torch.tensor(query, dtype=torch.float32)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = large_corpus_cpu @ query_cpu\n",
    "cpu_time = (time.time() - start) / 10\n",
    "\n",
    "# GPU timing (if not cpu)\n",
    "if device.type != 'cpu':\n",
    "    large_corpus_device = torch.tensor(large_corpus_np, device=device)\n",
    "    query_device = torch.tensor(query, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = large_corpus_device @ query_device\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = large_corpus_device @ query_device\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "    device_time = (time.time() - start) / 10\n",
    "    \n",
    "    print(f\"\\nBenchmark (100K × 384 matrix-vector product):\")\n",
    "    print(f\"  CPU: {cpu_time*1000:.2f} ms\")\n",
    "    print(f\"  {device.type.upper()}: {device_time*1000:.2f} ms\")\n",
    "    print(f\"  Speedup: {cpu_time/device_time:.1f}x\")\n",
    "else:\n",
    "    print(f\"\\nCPU-only timing: {cpu_time*1000:.2f} ms for 100K searches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# EXERCISES\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "These exercises extend what you've learned. Complete them to solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Batch Search\n",
    "\n",
    "Modify the search to handle multiple queries at once. Given a batch of 5 queries, compute the similarity of each query against all corpus embeddings.\n",
    "\n",
    "Expected output shape: (5, 1000) — each row is the similarities for one query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "# Create 5 random queries\n",
    "queries_raw = np.random.randn(5, embedding_dim)\n",
    "queries = queries_raw / np.linalg.norm(queries_raw, axis=1, keepdims=True)\n",
    "\n",
    "# Compute all similarities at once\n",
    "# Hint: (5, 384) @ (384, 1000) → (5, 1000)\n",
    "# You need to transpose the corpus: corpus.T\n",
    "batch_similarities = None  # TODO\n",
    "\n",
    "print(f\"Batch similarities shape: {batch_similarities.shape}\")\n",
    "print(f\"Query 0 top match: snippet {batch_similarities[0].argmax()} (sim: {batch_similarities[0].max():.4f})\")\n",
    "print(f\"Query 4 top match: snippet {batch_similarities[4].argmax()} (sim: {batch_similarities[4].max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Contrastive Loss\n",
    "\n",
    "Implement a simple contrastive loss: given a query and a \"positive\" snippet (the one we want to rank highly) and \"negative\" snippets (all others), maximize the margin between positive and negative similarities.\n",
    "\n",
    "Loss = max(0, margin - (sim_positive - sim_negative_mean))\n",
    "\n",
    "This loss is 0 when the positive snippet is sufficiently more similar than the average negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "\n",
    "def contrastive_loss(corpus_pt, query_pt, positive_idx, margin=0.5):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss.\n",
    "    \n",
    "    Args:\n",
    "        corpus_pt: (N, D) tensor of corpus embeddings\n",
    "        query_pt: (D,) tensor of query embedding\n",
    "        positive_idx: index of the \"correct\" snippet\n",
    "        margin: desired gap between positive and negative similarities\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # TODO: Compute similarities\n",
    "    # TODO: Get positive similarity (at positive_idx)\n",
    "    # TODO: Compute mean of negative similarities (all except positive_idx)\n",
    "    # TODO: Return max(0, margin - (positive - negative_mean))\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "corpus_fresh = torch.tensor(corpus, requires_grad=True, dtype=torch.float32)\n",
    "query_fresh = torch.tensor(query, dtype=torch.float32)\n",
    "loss = contrastive_loss(corpus_fresh, query_fresh, positive_idx=42)\n",
    "print(f\"Contrastive loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# OUTRO\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "![The search network works](hero-outro.png)\n",
    "\n",
    "## What Just Happened\n",
    "\n",
    "You built the computational backbone of a semantic code search system. Along the way, you:\n",
    "\n",
    "- Created high-dimensional embeddings as tensors\n",
    "- Used broadcasting to compute 1000 dot products in one line\n",
    "- Extracted top-k results using fancy indexing\n",
    "- Made embeddings learnable with autograd\n",
    "- Moved everything to GPU for speed\n",
    "\n",
    "These aren't toy examples. This is exactly how production semantic search works—just with learned embeddings instead of random ones.\n",
    "\n",
    "## What's Next\n",
    "\n",
    "In **01b: Linear Algebra Refresh**, we'll dig into *why* these operations work geometrically:\n",
    "- Vectors as arrows, matrices as transformations\n",
    "- Eigenvalues and why they matter for stability\n",
    "- SVD for finding the \"important directions\" in your embedding space\n",
    "\n",
    "The intuition you build there will pay off when we design actual embedding models in Week 2.\n",
    "\n",
    "→ [01b: Linear Algebra Refresh](01b-linear-algebra-core.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# RESOURCES\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "**Official Documentation**:\n",
    "- [PyTorch Tensors Tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "- [NumPy Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "- [PyTorch Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "**Deep Dives**:\n",
    "- [Jay Alammar: A Visual Intro to NumPy](https://jalammar.github.io/visual-numpy/)\n",
    "- [PyTorch Internals: Autograd](http://blog.ezyang.com/2019/05/pytorch-internals/)\n",
    "\n",
    "**Depth Notebook**:\n",
    "- [01a: NumPy & PyTorch — Depth](01a-numpy-pytorch-depth.ipynb) — CS terminology, complexity analysis, mathematical formalism"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
