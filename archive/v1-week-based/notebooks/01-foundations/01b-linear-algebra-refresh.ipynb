{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01b: Linear Algebra Refresh\n",
    "\n",
    "**Week 1, Days 3-4** | Foundations\n",
    "\n",
    "**Prerequisites**: 01a (tensors, broadcasting, indexing)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Visualize vectors as arrows in space\n",
    "- [ ] Understand matrices as transformations (rotation, scaling, shearing)\n",
    "- [ ] Compute and interpret eigenvalues and eigenvectors\n",
    "- [ ] Use SVD for dimensionality reduction\n",
    "- [ ] Connect these concepts to machine learning applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import torch\n",
    "\n",
    "# Config\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Vectors\n",
    "\n",
    "## Layer 1: Intuition â€” Arrows in space\n",
    "\n",
    "Forget lists of numbers for a moment. A **vector** is an arrow:\n",
    "\n",
    "- It has a **direction** (where it points)\n",
    "- It has a **magnitude** (how long it is)\n",
    "- It does NOT have a fixed position (you can slide it around)\n",
    "\n",
    "Think of a vector as a **displacement**: \"move 3 steps east and 4 steps north.\" It doesn't matter where you startâ€”the displacement is the same.\n",
    "\n",
    "**Physical analogies**:\n",
    "- **Velocity**: speed + direction (a car going 60 mph north)\n",
    "- **Force**: strength + direction (pushing a box with 10N to the right)\n",
    "- **Wind**: speed + direction\n",
    "\n",
    "In machine learning, we extend this to high dimensions:\n",
    "- A word embedding is a vector in 300-dimensional space\n",
    "- An image is a vector in (height Ã— width Ã— channels)-dimensional space\n",
    "- Each dimension captures some feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot vectors\n",
    "def plot_vectors(vectors, colors=None, labels=None, title=\"Vectors\", ax=None):\n",
    "    \"\"\"\n",
    "    Plot 2D vectors as arrows from the origin.\n",
    "    \n",
    "    Args:\n",
    "        vectors: List of 2D vectors (numpy arrays)\n",
    "        colors: Optional list of colors\n",
    "        labels: Optional list of labels\n",
    "        title: Plot title\n",
    "        ax: Optional matplotlib axis\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    if colors is None:\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(vectors)))\n",
    "    \n",
    "    # Find plot bounds\n",
    "    all_coords = np.array(vectors)\n",
    "    max_val = np.abs(all_coords).max() * 1.3\n",
    "    \n",
    "    for i, v in enumerate(vectors):\n",
    "        color = colors[i] if isinstance(colors, (list, np.ndarray)) else colors\n",
    "        label = labels[i] if labels else None\n",
    "        ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n",
    "                  color=color, width=0.02, label=label)\n",
    "    \n",
    "    ax.set_xlim(-max_val, max_val)\n",
    "    ax.set_ylim(-max_val, max_val)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(title)\n",
    "    if labels:\n",
    "        ax.legend()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize basic vector operations\n",
    "\n",
    "v = np.array([3, 2])\n",
    "w = np.array([1, 4])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Two vectors\n",
    "plot_vectors([v, w], colors=['blue', 'red'], labels=['v = [3,2]', 'w = [1,4]'],\n",
    "             title='Two vectors', ax=axes[0])\n",
    "\n",
    "# Plot 2: Vector addition (parallelogram rule)\n",
    "plot_vectors([v, w, v + w], colors=['blue', 'red', 'green'],\n",
    "             labels=['v', 'w', 'v + w'], title='Vector Addition', ax=axes[1])\n",
    "# Show the parallelogram\n",
    "axes[1].plot([v[0], v[0]+w[0]], [v[1], v[1]+w[1]], 'r--', alpha=0.5)\n",
    "axes[1].plot([w[0], v[0]+w[0]], [w[1], v[1]+w[1]], 'b--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Scalar multiplication\n",
    "plot_vectors([v, 2*v, -0.5*v], colors=['blue', 'green', 'orange'],\n",
    "             labels=['v', '2v', '-0.5v'], title='Scalar Multiplication', ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector magnitude and direction\n",
    "\n",
    "v = np.array([3, 4])\n",
    "\n",
    "# Magnitude (length): ||v|| = sqrt(vÂ·v)\n",
    "magnitude = np.linalg.norm(v)\n",
    "print(f\"v = {v}\")\n",
    "print(f\"||v|| = sqrt(3Â² + 4Â²) = {magnitude}\")\n",
    "\n",
    "# Unit vector (direction only): v / ||v||\n",
    "unit = v / magnitude\n",
    "print(f\"Unit vector: {unit}\")\n",
    "print(f\"Unit vector magnitude: {np.linalg.norm(unit)}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_vectors([v, unit*2], colors=['blue', 'red'],\n",
    "             labels=[f'v (||v||={magnitude})', 'unit vector (scaled 2x for visibility)'],\n",
    "             title='Vector and its unit vector', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak\n",
    "\n",
    "### Vector Operations and Complexity\n",
    "\n",
    "| Operation | Formula | Complexity | NumPy |\n",
    "|-----------|---------|------------|-------|\n",
    "| Addition | $\\mathbf{u} + \\mathbf{v}$ | O(n) | `u + v` |\n",
    "| Scalar mult | $c\\mathbf{v}$ | O(n) | `c * v` |\n",
    "| Dot product | $\\mathbf{u} \\cdot \\mathbf{v}$ | O(n) | `np.dot(u, v)` or `u @ v` |\n",
    "| Magnitude | $\\|\\mathbf{v}\\|$ | O(n) | `np.linalg.norm(v)` |\n",
    "| Normalize | $\\mathbf{v} / \\|\\mathbf{v}\\|$ | O(n) | `v / np.linalg.norm(v)` |\n",
    "\n",
    "### The Dot Product: Similarity Measure\n",
    "\n",
    "The dot product $\\mathbf{u} \\cdot \\mathbf{v}$ tells you:\n",
    "- **Positive**: vectors point in similar directions\n",
    "- **Zero**: vectors are perpendicular (orthogonal)\n",
    "- **Negative**: vectors point in opposite directions\n",
    "\n",
    "In ML, **cosine similarity** is just the dot product of unit vectors:\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}$$\n",
    "\n",
    "This is how we measure \"how similar are these two embeddings?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product and angle between vectors\n",
    "\n",
    "def angle_between(u, v):\n",
    "    \"\"\"Compute angle between two vectors in degrees.\"\"\"\n",
    "    cos_angle = np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    # Clamp to [-1, 1] to handle numerical errors\n",
    "    cos_angle = np.clip(cos_angle, -1, 1)\n",
    "    return np.degrees(np.arccos(cos_angle))\n",
    "\n",
    "# Examples\n",
    "pairs = [\n",
    "    (np.array([1, 0]), np.array([1, 0])),     # Same direction\n",
    "    (np.array([1, 0]), np.array([0, 1])),     # Perpendicular\n",
    "    (np.array([1, 0]), np.array([-1, 0])),    # Opposite\n",
    "    (np.array([1, 1]), np.array([1, 0])),     # 45 degrees\n",
    "]\n",
    "\n",
    "print(\"Dot product and angles:\\n\")\n",
    "for u, v in pairs:\n",
    "    dot = np.dot(u, v)\n",
    "    angle = angle_between(u, v)\n",
    "    print(f\"u={u}, v={v}\")\n",
    "    print(f\"  uÂ·v = {dot:.2f}, angle = {angle:.1f}Â°\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "### Definitions\n",
    "\n",
    "A **vector** $\\mathbf{v} \\in \\mathbb{R}^n$ is an ordered tuple of $n$ real numbers:\n",
    "\n",
    "$$\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}$$\n",
    "\n",
    "The **dot product** (inner product) of $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i = \\mathbf{u}^T \\mathbf{v}$$\n",
    "\n",
    "The **Euclidean norm** (length, magnitude):\n",
    "\n",
    "$$\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{\\sum_{i=1}^{n} v_i^2}$$\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "**Cauchy-Schwarz inequality**:\n",
    "$$|\\mathbf{u} \\cdot \\mathbf{v}| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|$$\n",
    "\n",
    "with equality iff $\\mathbf{u} = c\\mathbf{v}$ for some scalar $c$.\n",
    "\n",
    "**Geometric interpretation of dot product**:\n",
    "$$\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos(\\theta)$$\n",
    "\n",
    "where $\\theta$ is the angle between the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Matrices as Transformations\n",
    "\n",
    "## Layer 1: Intuition â€” Shape-shifting machines\n",
    "\n",
    "A matrix is not just a table of numbers. Think of it as a **transformation machine**:\n",
    "\n",
    "- You feed in a vector\n",
    "- It spits out a transformed vector\n",
    "\n",
    "What kinds of transformations? ALL linear ones:\n",
    "\n",
    "| Transformation | Visual | What it does |\n",
    "|----------------|--------|-------------|\n",
    "| **Rotation** | ðŸ”„ | Spins space around the origin |\n",
    "| **Scaling** | â†”ï¸â†•ï¸ | Stretches/compresses along axes |\n",
    "| **Reflection** | ðŸªž | Flips across a line |\n",
    "| **Shearing** | ðŸ“ | Tilts space (like italicizing text) |\n",
    "| **Projection** | ðŸ“½ï¸ | Flattens onto a subspace |\n",
    "\n",
    "**Key insight**: Matrix multiplication = applying a transformation. $\\mathbf{A}\\mathbf{v}$ means \"apply transformation $\\mathbf{A}$ to vector $\\mathbf{v}$.\"\n",
    "\n",
    "**Physical analogy**: Think of a fun-house mirror. Different mirrors (matrices) distort your reflection (vectors) in different ways. Some stretch you tall, some squish you wide, some twist you around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: visualize how a matrix transforms the unit circle\n",
    "\n",
    "def plot_transformation(A, title=\"Transformation\", ax=None):\n",
    "    \"\"\"\n",
    "    Show how matrix A transforms the unit circle and basis vectors.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Unit circle\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    circle = np.array([np.cos(theta), np.sin(theta)])\n",
    "    \n",
    "    # Transformed circle\n",
    "    transformed = A @ circle\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(circle[0], circle[1], 'b-', linewidth=2, label='Unit circle', alpha=0.5)\n",
    "    ax.plot(transformed[0], transformed[1], 'r-', linewidth=2, label='Transformed')\n",
    "    \n",
    "    # Basis vectors and their transforms\n",
    "    e1, e2 = np.array([1, 0]), np.array([0, 1])\n",
    "    Ae1, Ae2 = A @ e1, A @ e2\n",
    "    \n",
    "    ax.quiver(0, 0, e1[0], e1[1], angles='xy', scale_units='xy', scale=1,\n",
    "              color='blue', width=0.02, alpha=0.7, label='eâ‚ = [1,0]')\n",
    "    ax.quiver(0, 0, e2[0], e2[1], angles='xy', scale_units='xy', scale=1,\n",
    "              color='blue', width=0.02, alpha=0.7, label='eâ‚‚ = [0,1]')\n",
    "    ax.quiver(0, 0, Ae1[0], Ae1[1], angles='xy', scale_units='xy', scale=1,\n",
    "              color='red', width=0.02, label=f'Aeâ‚ = [{Ae1[0]:.1f},{Ae1[1]:.1f}]')\n",
    "    ax.quiver(0, 0, Ae2[0], Ae2[1], angles='xy', scale_units='xy', scale=1,\n",
    "              color='darkred', width=0.02, label=f'Aeâ‚‚ = [{Ae2[0]:.1f},{Ae2[1]:.1f}]')\n",
    "    \n",
    "    # Formatting\n",
    "    all_coords = np.concatenate([circle, transformed], axis=1)\n",
    "    max_val = np.abs(all_coords).max() * 1.2\n",
    "    ax.set_xlim(-max_val, max_val)\n",
    "    ax.set_ylim(-max_val, max_val)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_title(f\"{title}\\nA = {A.tolist()}\")\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common transformations\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Identity\n",
    "I = np.array([[1, 0], [0, 1]])\n",
    "plot_transformation(I, \"Identity\", axes[0, 0])\n",
    "\n",
    "# Scaling\n",
    "S = np.array([[2, 0], [0, 0.5]])\n",
    "plot_transformation(S, \"Scaling (2x horizontal, 0.5x vertical)\", axes[0, 1])\n",
    "\n",
    "# Rotation (45 degrees)\n",
    "theta = np.pi / 4\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "              [np.sin(theta), np.cos(theta)]])\n",
    "plot_transformation(R, \"Rotation (45Â°)\", axes[0, 2])\n",
    "\n",
    "# Shear\n",
    "H = np.array([[1, 0.5], [0, 1]])\n",
    "plot_transformation(H, \"Horizontal Shear\", axes[1, 0])\n",
    "\n",
    "# Reflection (across x-axis)\n",
    "F = np.array([[1, 0], [0, -1]])\n",
    "plot_transformation(F, \"Reflection (across x-axis)\", axes[1, 1])\n",
    "\n",
    "# Projection (onto x-axis)\n",
    "P = np.array([[1, 0], [0, 0]])\n",
    "plot_transformation(P, \"Projection (onto x-axis)\", axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication = composition of transformations\n",
    "\n",
    "# First rotate 45Â°, then scale\n",
    "theta = np.pi / 4\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "              [np.sin(theta), np.cos(theta)]])\n",
    "S = np.array([[2, 0], [0, 0.5]])\n",
    "\n",
    "# Composition: apply R first, then S\n",
    "# Note: S @ R means \"first R, then S\" (right-to-left)\n",
    "composed = S @ R\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "plot_transformation(R, \"Step 1: Rotate 45Â°\", axes[0])\n",
    "plot_transformation(S, \"Step 2: Scale\", axes[1])\n",
    "plot_transformation(composed, \"Combined: Scale(Rotate(Â·))\", axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Matrix multiplication is NOT commutative!\")\n",
    "print(f\"S @ R =\\n{S @ R}\")\n",
    "print(f\"R @ S =\\n{R @ S}\")\n",
    "print(f\"Equal? {np.allclose(S @ R, R @ S)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "For $\\mathbf{A} \\in \\mathbb{R}^{m \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times n}$:\n",
    "\n",
    "$$[\\mathbf{A}\\mathbf{B}]_{ij} = \\sum_{l=1}^{k} A_{il} B_{lj}$$\n",
    "\n",
    "**Complexity**: $O(m \\cdot k \\cdot n)$ â€” this is why large neural networks are expensive!\n",
    "\n",
    "### Matrix Properties\n",
    "\n",
    "| Property | Definition | Meaning |\n",
    "|----------|------------|---------|\n",
    "| **Symmetric** | $\\mathbf{A} = \\mathbf{A}^T$ | Same transformation and its \"reverse\" |\n",
    "| **Orthogonal** | $\\mathbf{A}^T \\mathbf{A} = \\mathbf{I}$ | Preserves lengths and angles (rotation/reflection) |\n",
    "| **Positive definite** | $\\mathbf{x}^T \\mathbf{A} \\mathbf{x} > 0$ | All eigenvalues positive |\n",
    "| **Invertible** | $\\det(\\mathbf{A}) \\neq 0$ | Transformation can be undone |\n",
    "\n",
    "### Key NumPy/PyTorch Functions\n",
    "\n",
    "```python\n",
    "A @ B           # Matrix multiplication\n",
    "A.T             # Transpose\n",
    "np.linalg.inv(A)   # Inverse\n",
    "np.linalg.det(A)   # Determinant\n",
    "np.trace(A)     # Trace (sum of diagonal)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "### Linear Transformations\n",
    "\n",
    "A function $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ is **linear** if:\n",
    "\n",
    "1. $T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$ (additivity)\n",
    "2. $T(c\\mathbf{v}) = cT(\\mathbf{v})$ (homogeneity)\n",
    "\n",
    "**Theorem**: Every linear transformation can be represented by a matrix.\n",
    "\n",
    "If $\\{\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\}$ is the standard basis, then:\n",
    "\n",
    "$$\\mathbf{A} = [T(\\mathbf{e}_1) | T(\\mathbf{e}_2) | \\cdots | T(\\mathbf{e}_n)]$$\n",
    "\n",
    "That is, **the columns of $\\mathbf{A}$ are where the basis vectors land**.\n",
    "\n",
    "### The Determinant\n",
    "\n",
    "The determinant $\\det(\\mathbf{A})$ measures how much the transformation scales area (2D) or volume (3D):\n",
    "\n",
    "$$|\\det(\\mathbf{A})| = \\frac{\\text{area of transformed unit square}}{\\text{area of unit square}}$$\n",
    "\n",
    "- $|\\det| > 1$: expansion\n",
    "- $|\\det| < 1$: compression\n",
    "- $|\\det| = 0$: collapses to lower dimension (not invertible)\n",
    "- $\\det < 0$: includes reflection (orientation flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinant = area scaling factor\n",
    "\n",
    "matrices = [\n",
    "    (np.array([[2, 0], [0, 2]]), \"Uniform scaling (2x)\"),\n",
    "    (np.array([[2, 0], [0, 0.5]]), \"Non-uniform scaling\"),\n",
    "    (np.array([[1, 1], [0, 1]]), \"Shear\"),\n",
    "    (np.array([[0, 1], [1, 0]]), \"Reflection\"),\n",
    "    (np.array([[1, 2], [2, 4]]), \"Singular (collapses)\"),\n",
    "]\n",
    "\n",
    "print(\"Matrix determinants and their meanings:\\n\")\n",
    "for A, name in matrices:\n",
    "    det = np.linalg.det(A)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  A = {A.tolist()}\")\n",
    "    print(f\"  det(A) = {det:.2f}\")\n",
    "    if det == 0:\n",
    "        print(f\"  â†’ Singular! Collapses space.\")\n",
    "    elif det < 0:\n",
    "        print(f\"  â†’ Includes reflection, scales area by {abs(det):.2f}x\")\n",
    "    else:\n",
    "        print(f\"  â†’ Scales area by {det:.2f}x\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Eigenvalues and Eigenvectors\n",
    "\n",
    "## Layer 1: Intuition â€” Special directions that only stretch\n",
    "\n",
    "Most vectors get rotated AND stretched when you apply a matrix. But some special vectors **only get stretched** (or flipped) â€” they keep pointing in the same direction.\n",
    "\n",
    "These are **eigenvectors**. The amount they stretch by is the **eigenvalue**.\n",
    "\n",
    "**Physical analogy**: Imagine pushing a playground merry-go-round:\n",
    "- Most points move in circles (rotation + radial motion)\n",
    "- But the center point stays put (eigenvector with eigenvalue 0)\n",
    "- And points along the axis of rotation only move up/down (eigenvector along the axis)\n",
    "\n",
    "**Why do we care?**\n",
    "- Eigenvectors reveal the \"natural axes\" of a transformation\n",
    "- PCA finds eigenvectors of the covariance matrix â†’ principal directions of variation\n",
    "- In dynamical systems, eigenvalues determine stability\n",
    "- Google's PageRank is based on finding the principal eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple eigenvector example\n",
    "\n",
    "A = np.array([[3, 1],\n",
    "              [0, 2]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\\n\")\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors (as columns):\\n{eigenvectors}\\n\")\n",
    "\n",
    "# Verify: A @ v = Î» @ v\n",
    "for i, (lam, v) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "    Av = A @ v\n",
    "    lam_v = lam * v\n",
    "    print(f\"Eigenvector {i+1}: v = {v}\")\n",
    "    print(f\"  A @ v  = {Av}\")\n",
    "    print(f\"  Î» * v  = {lam_v}\")\n",
    "    print(f\"  Equal? {np.allclose(Av, lam_v)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize eigenvectors: they only get scaled, not rotated\n",
    "\n",
    "A = np.array([[2, 1],\n",
    "              [1, 2]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Show transformation of random vectors\n",
    "ax = axes[0]\n",
    "n_vectors = 8\n",
    "angles = np.linspace(0, 2*np.pi, n_vectors, endpoint=False)\n",
    "vectors = np.array([[np.cos(a), np.sin(a)] for a in angles])\n",
    "\n",
    "for v in vectors:\n",
    "    Av = A @ v\n",
    "    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n",
    "              color='blue', alpha=0.5, width=0.01)\n",
    "    ax.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1,\n",
    "              color='red', alpha=0.5, width=0.01)\n",
    "\n",
    "# Highlight eigenvectors\n",
    "for i, (lam, v) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "    Av = A @ v\n",
    "    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n",
    "              color='blue', width=0.02, label=f'v{i+1} (before)' if i==0 else None)\n",
    "    ax.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1,\n",
    "              color='green', width=0.02, label=f'Av{i+1} = {lam:.1f}Â·v{i+1}' if i==0 else None)\n",
    "\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_title('Blue: original vectors, Red: transformed\\nGreen: eigenvectors (only scaled!)')\n",
    "ax.legend()\n",
    "\n",
    "# Right: Show eigenvalues on spectrum\n",
    "ax = axes[1]\n",
    "ax.axhline(y=0, color='k', linewidth=2)\n",
    "for i, lam in enumerate(eigenvalues):\n",
    "    ax.scatter(lam, 0, s=200, c=f'C{i}', zorder=5)\n",
    "    ax.annotate(f'Î»{i+1} = {lam:.2f}', (lam, 0.1), ha='center', fontsize=12)\n",
    "\n",
    "ax.set_xlim(-1, 4)\n",
    "ax.set_ylim(-0.5, 0.5)\n",
    "ax.set_xlabel('Eigenvalue')\n",
    "ax.set_title(f'Eigenvalue spectrum of A\\nA = {A.tolist()}')\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What eigenvalues tell us about repeated application\n",
    "\n",
    "# Matrix with |Î»| < 1: repeated application shrinks\n",
    "A_shrink = np.array([[0.8, 0.1],\n",
    "                      [0.1, 0.8]])\n",
    "\n",
    "# Matrix with |Î»| > 1: repeated application grows\n",
    "A_grow = np.array([[1.2, 0.1],\n",
    "                   [0.1, 1.2]])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, A, name in [(axes[0], A_shrink, 'Contracting'), \n",
    "                     (axes[1], A_grow, 'Expanding')]:\n",
    "    eigenvalues, _ = np.linalg.eig(A)\n",
    "    \n",
    "    # Start with a random point\n",
    "    v = np.array([1.0, 0.5])\n",
    "    trajectory = [v.copy()]\n",
    "    \n",
    "    # Apply A repeatedly\n",
    "    for _ in range(20):\n",
    "        v = A @ v\n",
    "        trajectory.append(v.copy())\n",
    "    \n",
    "    trajectory = np.array(trajectory)\n",
    "    \n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], 'b.-', markersize=8)\n",
    "    ax.scatter(trajectory[0, 0], trajectory[0, 1], c='green', s=100, zorder=5, label='Start')\n",
    "    ax.scatter(trajectory[-1, 0], trajectory[-1, 1], c='red', s=100, zorder=5, label='End')\n",
    "    \n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{name}: eigenvalues = {eigenvalues.round(2)}\\n'\n",
    "                 f'Max |Î»| = {np.abs(eigenvalues).max():.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: If max|Î»| < 1, trajectories converge to 0\")\n",
    "print(\"            If max|Î»| > 1, trajectories diverge to infinity\")\n",
    "print(\"            If max|Î»| = 1, trajectories stay bounded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak\n",
    "\n",
    "### Computing Eigenvalues\n",
    "\n",
    "```python\n",
    "# Full eigendecomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "# For symmetric matrices (more stable)\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(A)\n",
    "\n",
    "# Just eigenvalues\n",
    "eigenvalues = np.linalg.eigvals(A)\n",
    "```\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "If $\\mathbf{A}$ has $n$ linearly independent eigenvectors:\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{V}$ has eigenvectors as columns\n",
    "- $\\mathbf{\\Lambda}$ is diagonal with eigenvalues\n",
    "\n",
    "**Power**: This makes $\\mathbf{A}^n$ easy to compute:\n",
    "$$\\mathbf{A}^n = \\mathbf{V} \\mathbf{\\Lambda}^n \\mathbf{V}^{-1}$$\n",
    "\n",
    "### Applications in ML\n",
    "\n",
    "| Application | What we compute |\n",
    "|-------------|----------------|\n",
    "| **PCA** | Eigenvectors of covariance matrix |\n",
    "| **Spectral clustering** | Eigenvectors of Laplacian |\n",
    "| **PageRank** | Principal eigenvector of link matrix |\n",
    "| **Stability analysis** | Eigenvalues of Jacobian |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "### Definition\n",
    "\n",
    "For matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, scalar $\\lambda$ is an **eigenvalue** and non-zero vector $\\mathbf{v}$ is an **eigenvector** if:\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0}$$\n",
    "\n",
    "For non-trivial solutions, we need:\n",
    "\n",
    "$$\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0$$\n",
    "\n",
    "This is the **characteristic polynomial**, a degree-$n$ polynomial in $\\lambda$.\n",
    "\n",
    "### Properties\n",
    "\n",
    "**Trace**: $\\text{tr}(\\mathbf{A}) = \\sum_i \\lambda_i$ (sum of eigenvalues = sum of diagonal)\n",
    "\n",
    "**Determinant**: $\\det(\\mathbf{A}) = \\prod_i \\lambda_i$ (product of eigenvalues)\n",
    "\n",
    "**Symmetric matrices**: Real eigenvalues, orthogonal eigenvectors\n",
    "\n",
    "**Spectral theorem**: For symmetric $\\mathbf{A}$:\n",
    "$$\\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T$$\n",
    "\n",
    "where $\\mathbf{Q}$ is orthogonal ($\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Singular Value Decomposition (SVD)\n",
    "\n",
    "## Layer 1: Intuition â€” The ultimate matrix X-ray\n",
    "\n",
    "Eigendecomposition only works for square matrices. **SVD works for ANY matrix** and reveals its fundamental structure.\n",
    "\n",
    "Think of SVD as taking an X-ray of a matrix to see its \"bones\":\n",
    "\n",
    "1. **What directions in input space matter most?** (right singular vectors)\n",
    "2. **How much does each direction get stretched?** (singular values)\n",
    "3. **Where do they end up in output space?** (left singular vectors)\n",
    "\n",
    "**Physical analogy**: Imagine clay going through a pasta machine:\n",
    "- The machine can rotate the clay (V)\n",
    "- Stretch it in certain directions (Î£)\n",
    "- Then rotate the output (U)\n",
    "\n",
    "SVD finds the simplest decomposition: rotate â†’ stretch â†’ rotate.\n",
    "\n",
    "**Killer application**: **Low-rank approximation**. Keep only the top $k$ singular values to compress data while preserving the most important structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD basics\n",
    "\n",
    "A = np.array([[3, 2, 2],\n",
    "              [2, 3, -2]])\n",
    "\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "\n",
    "print(f\"Original A ({A.shape[0]}Ã—{A.shape[1]}):\")\n",
    "print(A)\n",
    "print(f\"\\nU ({U.shape[0]}Ã—{U.shape[1]}):  Left singular vectors\")\n",
    "print(U)\n",
    "print(f\"\\nS ({S.shape}):  Singular values\")\n",
    "print(S)\n",
    "print(f\"\\nVáµ€ ({Vt.shape[0]}Ã—{Vt.shape[1]}):  Right singular vectors (transposed)\")\n",
    "print(Vt)\n",
    "\n",
    "# Reconstruct\n",
    "S_matrix = np.zeros_like(A, dtype=float)\n",
    "np.fill_diagonal(S_matrix, S)\n",
    "A_reconstructed = U @ S_matrix @ Vt\n",
    "print(f\"\\nReconstruction U @ Î£ @ Váµ€:\")\n",
    "print(A_reconstructed)\n",
    "print(f\"\\nReconstruction error: {np.linalg.norm(A - A_reconstructed):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVD as rotation-stretch-rotation\n",
    "\n",
    "A = np.array([[2, 1],\n",
    "              [1, 2]])\n",
    "\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Unit circle\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "circle = np.array([np.cos(theta), np.sin(theta)])\n",
    "\n",
    "# Step 1: Original\n",
    "ax = axes[0]\n",
    "ax.plot(circle[0], circle[1], 'b-', linewidth=2)\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('1. Unit circle\\n(input)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Step 2: After Váµ€ (rotation)\n",
    "after_Vt = Vt @ circle\n",
    "ax = axes[1]\n",
    "ax.plot(after_Vt[0], after_Vt[1], 'g-', linewidth=2)\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('2. After Váµ€\\n(rotate in input space)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Step 3: After Î£ (stretch)\n",
    "after_S = np.diag(S) @ after_Vt\n",
    "ax = axes[2]\n",
    "ax.plot(after_S[0], after_S[1], 'm-', linewidth=2)\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title(f'3. After Î£\\n(stretch by Ïƒâ‚={S[0]:.2f}, Ïƒâ‚‚={S[1]:.2f})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Step 4: After U (rotation)\n",
    "after_U = U @ after_S\n",
    "ax = axes[3]\n",
    "ax.plot(after_U[0], after_U[1], 'r-', linewidth=2)\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('4. After U\\n(rotate in output space = final)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"A = U @ Î£ @ Váµ€\")\n",
    "print(f\"Any matrix = (rotation) @ (stretch) @ (rotation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-rank approximation: compress a matrix\n",
    "\n",
    "# Create a \"signal + noise\" matrix\n",
    "np.random.seed(42)\n",
    "m, n = 100, 50\n",
    "true_rank = 5\n",
    "\n",
    "# Low-rank signal\n",
    "signal = np.random.randn(m, true_rank) @ np.random.randn(true_rank, n)\n",
    "# Add noise\n",
    "noise = 0.5 * np.random.randn(m, n)\n",
    "A = signal + noise\n",
    "\n",
    "print(f\"Matrix A: {A.shape}\")\n",
    "print(f\"True underlying rank: {true_rank}\")\n",
    "print(f\"With noise, numerical rank: {np.linalg.matrix_rank(A)}\")\n",
    "\n",
    "# SVD\n",
    "U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "# Plot singular values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(S, 'bo-', markersize=4)\n",
    "ax.axhline(y=S[true_rank-1], color='r', linestyle='--', label=f'Ïƒ_{true_rank} (signal cutoff)')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Singular value')\n",
    "ax.set_title('Singular value spectrum\\n(signal lives in top values)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction error vs rank\n",
    "ax = axes[1]\n",
    "ranks = range(1, 20)\n",
    "errors = []\n",
    "for k in ranks:\n",
    "    A_k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n",
    "    error = np.linalg.norm(A - A_k) / np.linalg.norm(A)\n",
    "    errors.append(error)\n",
    "\n",
    "ax.plot(ranks, errors, 'go-', markersize=6)\n",
    "ax.axvline(x=true_rank, color='r', linestyle='--', label=f'True rank ({true_rank})')\n",
    "ax.set_xlabel('Rank k')\n",
    "ax.set_ylabel('Relative reconstruction error')\n",
    "ax.set_title('Low-rank approximation error\\n(elbow at true rank)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Energy captured\n",
    "total_energy = np.sum(S**2)\n",
    "cumulative_energy = np.cumsum(S**2) / total_energy\n",
    "print(f\"\\nEnergy captured by top k singular values:\")\n",
    "for k in [1, 5, 10, 20]:\n",
    "    print(f\"  k={k}: {cumulative_energy[k-1]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak\n",
    "\n",
    "### SVD Decomposition\n",
    "\n",
    "For any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$:\n",
    "\n",
    "```python\n",
    "U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "# U: (m, k) left singular vectors\n",
    "# S: (k,) singular values (sorted descending)\n",
    "# Vt: (k, n) right singular vectors (transposed)\n",
    "# where k = min(m, n)\n",
    "```\n",
    "\n",
    "### Low-Rank Approximation\n",
    "\n",
    "The **rank-k approximation** keeps only the top k components:\n",
    "\n",
    "```python\n",
    "A_k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n",
    "```\n",
    "\n",
    "**Eckart-Young theorem**: This is the BEST rank-k approximation (minimizes Frobenius norm error).\n",
    "\n",
    "### Applications\n",
    "\n",
    "| Application | How SVD is used |\n",
    "|-------------|----------------|\n",
    "| **Image compression** | Keep top k singular values |\n",
    "| **Recommender systems** | Low-rank matrix factorization |\n",
    "| **LSA (text)** | Dimensionality reduction on term-document matrix |\n",
    "| **Pseudoinverse** | $\\mathbf{A}^+ = \\mathbf{V}\\mathbf{\\Sigma}^+\\mathbf{U}^T$ |\n",
    "| **PCA** | SVD of centered data matrix |\n",
    "\n",
    "### Complexity\n",
    "\n",
    "Full SVD: $O(\\min(mn^2, m^2n))$ â€” expensive for large matrices!\n",
    "\n",
    "Truncated SVD (top k): $O(mnk)$ â€” use `scipy.sparse.linalg.svds` for large sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "### Theorem (SVD)\n",
    "\n",
    "For any $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ with rank $r$, there exist:\n",
    "- Orthogonal $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$\n",
    "- Orthogonal $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$\n",
    "- Diagonal $\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times n}$ with $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r > 0$\n",
    "\n",
    "such that:\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "### Relationship to Eigendecomposition\n",
    "\n",
    "- Left singular vectors $\\mathbf{u}_i$ are eigenvectors of $\\mathbf{A}\\mathbf{A}^T$\n",
    "- Right singular vectors $\\mathbf{v}_i$ are eigenvectors of $\\mathbf{A}^T\\mathbf{A}$\n",
    "- Singular values: $\\sigma_i = \\sqrt{\\lambda_i}$ where $\\lambda_i$ are eigenvalues of $\\mathbf{A}^T\\mathbf{A}$\n",
    "\n",
    "### Eckart-Young-Mirsky Theorem\n",
    "\n",
    "The rank-$k$ truncated SVD:\n",
    "\n",
    "$$\\mathbf{A}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "minimizes $\\|\\mathbf{A} - \\mathbf{B}\\|_F$ over all rank-$k$ matrices $\\mathbf{B}$.\n",
    "\n",
    "The error is:\n",
    "\n",
    "$$\\|\\mathbf{A} - \\mathbf{A}_k\\|_F = \\sqrt{\\sigma_{k+1}^2 + \\cdots + \\sigma_r^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Now it's your turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Transformation Visualization\n",
    "\n",
    "Create a 2Ã—2 matrix that:\n",
    "1. First stretches by 2x horizontally\n",
    "2. Then rotates by 30 degrees\n",
    "\n",
    "Plot the unit circle and its transformation. Verify your matrix is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Transformation Visualization\n",
    "\n",
    "def create_stretch_rotate_matrix(stretch_x, angle_degrees):\n",
    "    \"\"\"\n",
    "    Create a matrix that first stretches horizontally, then rotates.\n",
    "    \n",
    "    Args:\n",
    "        stretch_x: Horizontal stretch factor\n",
    "        angle_degrees: Rotation angle in degrees\n",
    "    \n",
    "    Returns:\n",
    "        2x2 transformation matrix\n",
    "    \"\"\"\n",
    "    # TODO: Create stretch matrix [[stretch_x, 0], [0, 1]]\n",
    "    # TODO: Create rotation matrix\n",
    "    # TODO: Compose them (remember: right-to-left order!)\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "A = create_stretch_rotate_matrix(2, 30)\n",
    "print(f\"Transformation matrix:\\n{A}\")\n",
    "\n",
    "# Visualize\n",
    "plot_transformation(A, \"Stretch 2x, then rotate 30Â°\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Eigenvalue Intuition\n",
    "\n",
    "Find a 2Ã—2 matrix where BOTH eigenvalues are negative. What happens to vectors under repeated application of this matrix? Visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Eigenvalue Intuition\n",
    "\n",
    "def create_negative_eigenvalue_matrix():\n",
    "    \"\"\"\n",
    "    Create a 2x2 matrix with both eigenvalues negative.\n",
    "    \n",
    "    Hint: A diagonal matrix [[a, 0], [0, b]] has eigenvalues a and b.\n",
    "    Or use the fact that reflection has eigenvalue -1.\n",
    "    \n",
    "    Returns:\n",
    "        2x2 numpy array with both eigenvalues < 0\n",
    "    \"\"\"\n",
    "    # TODO: Create your matrix\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "A = create_negative_eigenvalue_matrix()\n",
    "eigenvalues = np.linalg.eigvals(A)\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Both negative? {all(eigenvalues < 0)}\")\n",
    "\n",
    "# TODO: Visualize trajectory under repeated application\n",
    "# What pattern do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: SVD Experiment\n",
    "\n",
    "Take a 100Ã—50 matrix of random numbers. Use SVD to find the rank-10 approximation. \n",
    "\n",
    "Questions:\n",
    "1. How much \"energy\" (sum of squared singular values) is preserved?\n",
    "2. What is the compression ratio (original size vs. stored components)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: SVD Experiment\n",
    "\n",
    "def svd_analysis(m, n, k):\n",
    "    \"\"\"\n",
    "    Analyze rank-k SVD approximation of a random mÃ—n matrix.\n",
    "    \n",
    "    Args:\n",
    "        m, n: Matrix dimensions\n",
    "        k: Target rank\n",
    "    \n",
    "    Returns:\n",
    "        Dict with analysis results\n",
    "    \"\"\"\n",
    "    # TODO: Create random matrix\n",
    "    # TODO: Compute SVD\n",
    "    # TODO: Compute rank-k approximation\n",
    "    # TODO: Calculate:\n",
    "    #   - Energy preserved (sum of top k squared singular values / total)\n",
    "    #   - Reconstruction error\n",
    "    #   - Compression ratio (m*n original vs. m*k + k + k*n for SVD components)\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "results = svd_analysis(100, 50, 10)\n",
    "print(f\"Results for 100Ã—50 matrix, rank-10 approximation:\")\n",
    "print(f\"  Energy preserved: {results['energy_preserved']*100:.1f}%\")\n",
    "print(f\"  Relative error: {results['relative_error']:.3f}\")\n",
    "print(f\"  Compression ratio: {results['compression_ratio']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Why This Matters\n",
    "\n",
    "Linear algebra is the language of machine learning:\n",
    "\n",
    "| Concept | ML Application |\n",
    "|---------|---------------|\n",
    "| **Vectors** | Embeddings, feature vectors, gradients |\n",
    "| **Dot product** | Similarity, attention scores |\n",
    "| **Matrix multiplication** | Layer transformations, attention |\n",
    "| **Eigenvalues** | PCA, stability analysis, spectral methods |\n",
    "| **SVD** | Dimensionality reduction, compression, recommendations |\n",
    "\n",
    "In Week 2, we'll see embeddings as vectors in high-dimensional space. The intuition from 2D carries over: similar meanings = similar directions.\n",
    "\n",
    "In Weeks 5-6, eigenvalues will determine whether dynamical systems converge or divergeâ€”critical for understanding optimization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "**Essential** (watch these!):\n",
    "- [3Blue1Brown: Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) â€” Best visual intuition\n",
    "\n",
    "**Deep Dives**:\n",
    "- [Gilbert Strang: Linear Algebra (MIT OCW)](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/)\n",
    "- [Immersive Linear Algebra](http://immersivemath.com/ila/index.html) â€” Interactive textbook\n",
    "\n",
    "**Wiki**:\n",
    "- [Glossary](../../wiki/glossary.md) â€” eigenvalue, eigenvector, SVD, rank\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection Questions\n",
    "\n",
    "Before moving on:\n",
    "\n",
    "1. **What's the geometric interpretation of eigenvalues?**\n",
    "   - Think about what happens to eigenvectors under the transformation...\n",
    "\n",
    "2. **Why is SVD more general than eigendecomposition?**\n",
    "   - What kinds of matrices can each handle?\n",
    "\n",
    "3. **How would you use SVD to compress an image?**\n",
    "   - Think about treating the image as a matrix...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Up\n",
    "\n",
    "In **01c: Calculus Refresh**, we'll cover:\n",
    "- Derivatives and gradients\n",
    "- The chain rule (backbone of backpropagation)\n",
    "- Gradient descent from scratch\n",
    "\n",
    "This is where all the pieces come together for optimization.\n",
    "\n",
    "â†’ [01c: Calculus Refresh](01c-calculus-refresh.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
