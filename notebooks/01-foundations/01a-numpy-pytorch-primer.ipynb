{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01a: NumPy & PyTorch Primer\n",
    "\n",
    "**Week 1, Days 1-2** | Foundations\n",
    "\n",
    "**Prerequisites**: Basic Python (you're an expert, so ✓)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Create and manipulate tensors in NumPy and PyTorch\n",
    "- [ ] Understand and apply broadcasting rules\n",
    "- [ ] Use advanced indexing to reshape and slice data\n",
    "- [ ] Compute gradients automatically with autograd\n",
    "- [ ] Move tensors between CPU and GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Config\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS (Apple Silicon) available: True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Tensors\n",
    "\n",
    "## Layer 1: Intuition — What *is* a tensor?\n",
    "\n",
    "Imagine organizing data in space:\n",
    "\n",
    "- **A single number** (like temperature: 72°F) is a **scalar** — a point with no extent\n",
    "- **A list of numbers** (temperatures across a week: [72, 68, 75, ...]) is a **vector** — a line of values\n",
    "- **A grid of numbers** (temperatures across cities and days) is a **matrix** — a 2D sheet\n",
    "- **A cube of numbers** (temperatures across cities, days, and times) is a **3D tensor** — a volume\n",
    "\n",
    "And it keeps going: 4D, 5D, ... nD. A **tensor** is just an n-dimensional array of numbers.\n",
    "\n",
    "```\n",
    "0D: scalar       → point          → shape: ()\n",
    "1D: vector       → line           → shape: (n,)\n",
    "2D: matrix       → sheet          → shape: (m, n)\n",
    "3D: 3-tensor     → cube           → shape: (l, m, n)\n",
    "4D: 4-tensor     → batch of cubes → shape: (b, l, m, n)\n",
    "```\n",
    "\n",
    "**Key insight**: The \"rank\" of a tensor is just how many indices you need to grab a single number.\n",
    "A matrix needs two indices (row, column). A 3D tensor needs three.\n",
    "\n",
    "### Why tensors for ML?\n",
    "\n",
    "Everything in machine learning is a tensor:\n",
    "- An image is a 3D tensor: (height, width, channels)\n",
    "- A batch of images is 4D: (batch, height, width, channels)\n",
    "- A sentence is a 2D tensor: (sequence_length, embedding_dim)\n",
    "- Attention weights are 4D: (batch, heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors of different ranks\n",
    "\n",
    "# 0D: Scalar\n",
    "scalar_np = np.array(3.14)\n",
    "scalar_pt = torch.tensor(3.14)\n",
    "print(f\"Scalar: {scalar_np}, shape: {scalar_np.shape}, ndim: {scalar_np.ndim}\")\n",
    "\n",
    "# 1D: Vector\n",
    "vector_np = np.array([1, 2, 3, 4, 5])\n",
    "vector_pt = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"Vector: {vector_np}, shape: {vector_np.shape}, ndim: {vector_np.ndim}\")\n",
    "\n",
    "# 2D: Matrix\n",
    "matrix_np = np.array([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "matrix_pt = torch.tensor([[1, 2, 3],\n",
    "                          [4, 5, 6]])\n",
    "print(f\"Matrix shape: {matrix_np.shape}, ndim: {matrix_np.ndim}\")\n",
    "print(matrix_np)\n",
    "\n",
    "# 3D: Batch of matrices\n",
    "batch_np = np.random.randn(4, 3, 3)  # 4 matrices, each 3×3\n",
    "batch_pt = torch.randn(4, 3, 3)\n",
    "print(f\"\\n3D tensor shape: {batch_np.shape}, ndim: {batch_np.ndim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing tensor dimensions\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "\n",
    "# 0D: Scalar as a point\n",
    "axes[0].scatter([0], [0], s=200, c='blue')\n",
    "axes[0].set_xlim(-1, 1)\n",
    "axes[0].set_ylim(-1, 1)\n",
    "axes[0].set_title('0D: Scalar\\nshape: ()', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# 1D: Vector as a line of points\n",
    "axes[1].scatter(range(5), [0]*5, s=100, c='blue')\n",
    "axes[1].set_xlim(-0.5, 4.5)\n",
    "axes[1].set_ylim(-1, 1)\n",
    "axes[1].set_title('1D: Vector\\nshape: (5,)', fontsize=12)\n",
    "\n",
    "# 2D: Matrix as a grid\n",
    "im = axes[2].imshow(np.random.randn(4, 6), cmap='viridis', aspect='auto')\n",
    "axes[2].set_title('2D: Matrix\\nshape: (4, 6)', fontsize=12)\n",
    "\n",
    "# 3D: Show as stacked matrices\n",
    "for i in range(3):\n",
    "    offset = i * 0.3\n",
    "    rect = plt.Rectangle((offset, offset), 1, 1, fill=True, \n",
    "                          facecolor=plt.cm.viridis(i/3), alpha=0.7, \n",
    "                          edgecolor='black', linewidth=2)\n",
    "    axes[3].add_patch(rect)\n",
    "axes[3].set_xlim(-0.2, 1.8)\n",
    "axes[3].set_ylim(-0.2, 1.8)\n",
    "axes[3].set_title('3D: Stack of matrices\\nshape: (3, m, n)', fontsize=12)\n",
    "axes[3].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak\n",
    "\n",
    "### Terminology\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| **Rank** (ndim) | Number of dimensions/axes |\n",
    "| **Shape** | Tuple of sizes along each dimension |\n",
    "| **Size** (numel) | Total number of elements (product of shape) |\n",
    "| **dtype** | Data type of elements (float32, int64, etc.) |\n",
    "| **Stride** | Memory step size to move along each dimension |\n",
    "\n",
    "### Memory Layout\n",
    "\n",
    "Tensors are stored as contiguous blocks in memory. A 2×3 matrix:\n",
    "\n",
    "```\n",
    "[[a, b, c],     →  Memory: [a, b, c, d, e, f]\n",
    " [d, e, f]]         (row-major / C-contiguous)\n",
    "```\n",
    "\n",
    "**Strides** tell you how many elements to skip in memory to move one step along each axis:\n",
    "- Shape (2, 3) with row-major layout → strides (3, 1)\n",
    "- To go down one row: skip 3 elements\n",
    "- To go right one column: skip 1 element\n",
    "\n",
    "### NumPy vs PyTorch\n",
    "\n",
    "| Feature | NumPy | PyTorch |\n",
    "|---------|-------|--------|\n",
    "| Array type | `ndarray` | `Tensor` |\n",
    "| GPU support | No (need CuPy) | Yes (CUDA, MPS) |\n",
    "| Autograd | No | Yes |\n",
    "| Ecosystem | Data science | Deep learning |\n",
    "| Interop | `.numpy()` from torch | `torch.from_numpy()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring tensor attributes\n",
    "\n",
    "t = torch.randn(2, 3, 4)  # Random 2×3×4 tensor\n",
    "\n",
    "print(f\"Shape: {t.shape}\")\n",
    "print(f\"Rank (ndim): {t.ndim}\")\n",
    "print(f\"Total elements: {t.numel()}\")\n",
    "print(f\"Data type: {t.dtype}\")\n",
    "print(f\"Device: {t.device}\")\n",
    "print(f\"Strides: {t.stride()}\")\n",
    "print(f\"Is contiguous: {t.is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "### Definition\n",
    "\n",
    "A **tensor** of rank $n$ over a field $\\mathbb{F}$ (typically $\\mathbb{R}$) is a multilinear map:\n",
    "\n",
    "$$T: \\underbrace{V^* \\times \\cdots \\times V^*}_{p \\text{ copies}} \\times \\underbrace{V \\times \\cdots \\times V}_{q \\text{ copies}} \\to \\mathbb{F}$$\n",
    "\n",
    "where $V$ is a vector space and $V^*$ its dual. This is the physicist's definition.\n",
    "\n",
    "**For our purposes** (the ML practitioner's definition): A tensor is simply a multidimensional array:\n",
    "\n",
    "$$T \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_n}$$\n",
    "\n",
    "with elements accessed by $n$ indices:\n",
    "\n",
    "$$T_{i_1, i_2, \\ldots, i_n} \\in \\mathbb{R}$$\n",
    "\n",
    "### Notation\n",
    "\n",
    "- Scalars: lowercase ($x, y, \\alpha$)\n",
    "- Vectors: bold lowercase ($\\mathbf{x}, \\mathbf{v}$) or with arrow ($\\vec{x}$)\n",
    "- Matrices: bold uppercase ($\\mathbf{A}, \\mathbf{W}$)\n",
    "- General tensors: calligraphic ($\\mathcal{T}, \\mathcal{X}$) or bold with rank annotation ($\\mathbf{T}^{(n)}$)\n",
    "\n",
    "### Shape Algebra\n",
    "\n",
    "For $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times p}$:\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{B} \\in \\mathbb{R}^{m \\times p}$$\n",
    "\n",
    "The inner dimensions must match; the outer dimensions form the result shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Broadcasting\n",
    "\n",
    "## Layer 1: Intuition — Stretching to fit\n",
    "\n",
    "Imagine you have a photo (a 3D tensor: height × width × RGB) and you want to brighten it. \n",
    "Brightness is a single number, but somehow `photo + 0.1` just works. How?\n",
    "\n",
    "**Broadcasting** automatically \"stretches\" smaller arrays to match larger ones, without actually copying data.\n",
    "\n",
    "Think of it like a rubber stamp. If you have:\n",
    "- A 4×4 grid of values\n",
    "- A single row of 4 values\n",
    "\n",
    "Broadcasting \"stamps\" that row across all 4 rows of the grid:\n",
    "\n",
    "```\n",
    "Grid (4×4):     Row (1×4):      Result:\n",
    "[a a a a]       [x y z w]  →    [a+x a+y a+z a+w]\n",
    "[b b b b]   +                   [b+x b+y b+z b+w]\n",
    "[c c c c]                       [c+x c+y c+z c+w]\n",
    "[d d d d]                       [d+x d+y d+z d+w]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting in action\n",
    "\n",
    "# Scalar + matrix: scalar broadcasts everywhere\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "print(\"Matrix + 10:\")\n",
    "print(matrix + 10)\n",
    "print()\n",
    "\n",
    "# Row vector + matrix: row broadcasts down\n",
    "row = np.array([100, 200, 300])\n",
    "print(f\"Matrix shape: {matrix.shape}, Row shape: {row.shape}\")\n",
    "print(\"Matrix + row:\")\n",
    "print(matrix + row)\n",
    "print()\n",
    "\n",
    "# Column vector + matrix: column broadcasts across\n",
    "col = np.array([[10],\n",
    "                [20]])\n",
    "print(f\"Matrix shape: {matrix.shape}, Column shape: {col.shape}\")\n",
    "print(\"Matrix + column:\")\n",
    "print(matrix + col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing broadcasting\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "\n",
    "# Example 1: Matrix + scalar\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = 10\n",
    "\n",
    "axes[0, 0].imshow(A, cmap='Blues', vmin=0, vmax=15)\n",
    "axes[0, 0].set_title('A (2×2)')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0, 0].text(j, i, f'{A[i,j]}', ha='center', va='center', fontsize=14)\n",
    "\n",
    "axes[0, 1].text(0.5, 0.5, '+', fontsize=30, ha='center', va='center')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow([[b]], cmap='Oranges', vmin=0, vmax=15)\n",
    "axes[0, 2].set_title('Scalar: 10')\n",
    "axes[0, 2].text(0, 0, f'{b}', ha='center', va='center', fontsize=14)\n",
    "\n",
    "result1 = A + b\n",
    "axes[0, 3].imshow(result1, cmap='Greens', vmin=0, vmax=15)\n",
    "axes[0, 3].set_title('Result (2×2)')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0, 3].text(j, i, f'{result1[i,j]}', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Example 2: Matrix + row\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "row = np.array([100, 200, 300])\n",
    "\n",
    "axes[1, 0].imshow(A, cmap='Blues')\n",
    "axes[1, 0].set_title('A (3×3)')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1, 0].text(j, i, f'{A[i,j]}', ha='center', va='center', fontsize=12)\n",
    "\n",
    "axes[1, 1].text(0.5, 0.5, '+', fontsize=30, ha='center', va='center')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(row.reshape(1, -1), cmap='Oranges', aspect='auto')\n",
    "axes[1, 2].set_title('Row (1×3)')\n",
    "for j in range(3):\n",
    "    axes[1, 2].text(j, 0, f'{row[j]}', ha='center', va='center', fontsize=12)\n",
    "\n",
    "result2 = A + row\n",
    "axes[1, 3].imshow(result2, cmap='Greens')\n",
    "axes[1, 3].set_title('Result (3×3)')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1, 3].text(j, i, f'{result2[i,j]}', ha='center', va='center', fontsize=10)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak — The Broadcasting Rules\n",
    "\n",
    "NumPy/PyTorch broadcasting follows these rules:\n",
    "\n",
    "1. **Right-align shapes** and compare dimensions from the right\n",
    "2. **Dimensions are compatible** if they're equal OR one of them is 1\n",
    "3. **Missing dimensions** (on the left) are treated as 1\n",
    "\n",
    "```\n",
    "Shape A:     (8, 1, 6, 1)\n",
    "Shape B:        (7, 1, 5)\n",
    "                --------\n",
    "Right-align: (8, 1, 6, 1)\n",
    "             (1, 7, 1, 5)  ← implicit 1 on left\n",
    "                --------\n",
    "Result:      (8, 7, 6, 5)  ← max of each dimension\n",
    "```\n",
    "\n",
    "**Common patterns**:\n",
    "\n",
    "| Operation | Shape A | Shape B | Result | Use case |\n",
    "|-----------|---------|---------|--------|----------|\n",
    "| Scalar op | (m, n) | () | (m, n) | Add constant |\n",
    "| Row-wise | (m, n) | (n,) | (m, n) | Mean subtraction |\n",
    "| Col-wise | (m, n) | (m, 1) | (m, n) | Normalize by row sum |\n",
    "| Outer product | (m, 1) | (n,) | (m, n) | All pairs |\n",
    "\n",
    "**Efficiency note**: Broadcasting is memory-efficient. No data is actually copied—the computation just \"pretends\" the smaller array is repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting rule demonstration\n",
    "\n",
    "def check_broadcast(shape_a, shape_b):\n",
    "    \"\"\"Check if two shapes can be broadcast together.\"\"\"\n",
    "    a = np.zeros(shape_a)\n",
    "    b = np.zeros(shape_b)\n",
    "    try:\n",
    "        result = a + b\n",
    "        return f\"{shape_a} + {shape_b} → {result.shape}\"\n",
    "    except ValueError as e:\n",
    "        return f\"{shape_a} + {shape_b} → ERROR: {e}\"\n",
    "\n",
    "# These work\n",
    "print(check_broadcast((3, 4), (4,)))       # Row broadcast\n",
    "print(check_broadcast((3, 4), (3, 1)))     # Column broadcast\n",
    "print(check_broadcast((3, 4), (1, 4)))     # Explicit row broadcast\n",
    "print(check_broadcast((3, 1, 4), (1, 5, 4)))  # Middle dimension\n",
    "print()\n",
    "\n",
    "# This fails\n",
    "print(check_broadcast((3, 4), (3,)))       # Incompatible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "Broadcasting can be formalized as implicit replication along singleton dimensions.\n",
    "\n",
    "Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^{n}$ (a row vector).\n",
    "\n",
    "The operation $\\mathbf{A} + \\mathbf{b}$ is mathematically equivalent to:\n",
    "\n",
    "$$[\\mathbf{A} + \\mathbf{b}]_{ij} = A_{ij} + b_j$$\n",
    "\n",
    "Or equivalently, using the all-ones vector $\\mathbf{1}_m \\in \\mathbb{R}^m$:\n",
    "\n",
    "$$\\mathbf{A} + \\mathbf{b} = \\mathbf{A} + \\mathbf{1}_m \\mathbf{b}^T$$\n",
    "\n",
    "where $\\mathbf{1}_m \\mathbf{b}^T$ is an $m \\times n$ matrix with $\\mathbf{b}$ repeated in each row.\n",
    "\n",
    "**Outer product via broadcasting**:\n",
    "\n",
    "For $\\mathbf{u} \\in \\mathbb{R}^m$ and $\\mathbf{v} \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$\\mathbf{u} \\mathbf{v}^T = \\mathbf{u}_{:, \\text{None}} \\cdot \\mathbf{v}_{\\text{None}, :}$$\n",
    "\n",
    "where subscript `None` denotes inserting a singleton dimension (NumPy: `np.newaxis`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer product via broadcasting\n",
    "\n",
    "u = np.array([1, 2, 3, 4])  # Shape (4,)\n",
    "v = np.array([10, 20, 30])  # Shape (3,)\n",
    "\n",
    "# Method 1: Explicit outer product\n",
    "outer1 = np.outer(u, v)\n",
    "print(\"np.outer(u, v):\")\n",
    "print(outer1)\n",
    "print()\n",
    "\n",
    "# Method 2: Broadcasting\n",
    "outer2 = u[:, np.newaxis] * v[np.newaxis, :]  # (4, 1) * (1, 3) → (4, 3)\n",
    "print(\"Broadcasting (4,1) * (1,3):\")\n",
    "print(outer2)\n",
    "print()\n",
    "\n",
    "# Method 3: Even simpler broadcasting (numpy infers)\n",
    "outer3 = u[:, None] * v  # (4, 1) * (3,) → (4, 3)\n",
    "print(\"Same result:\", np.allclose(outer1, outer2, outer3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Indexing\n",
    "\n",
    "## Layer 1: Intuition — Slicing and dicing\n",
    "\n",
    "Think of a tensor as a multidimensional spreadsheet. Indexing lets you:\n",
    "\n",
    "- **Select cells**: `A[2, 3]` — grab one element\n",
    "- **Select rows/columns**: `A[2, :]` — grab row 2\n",
    "- **Select ranges**: `A[1:4, 0:2]` — grab a submatrix\n",
    "- **Select with conditions**: `A[A > 5]` — grab all elements > 5\n",
    "- **Fancy indexing**: `A[[0, 2, 4], :]` — grab rows 0, 2, 4\n",
    "\n",
    "**Key insight**: Python indexing is 0-based and uses `[start:stop:step]` where:\n",
    "- `start` is inclusive\n",
    "- `stop` is exclusive\n",
    "- Negatives count from the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a demo matrix\n",
    "A = np.arange(20).reshape(4, 5)\n",
    "print(\"Original matrix A:\")\n",
    "print(A)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic indexing\n",
    "print(\"Single element A[1, 2]:\", A[1, 2])  # Row 1, Col 2\n",
    "print()\n",
    "\n",
    "print(\"Row 2: A[2, :]\")\n",
    "print(A[2, :])\n",
    "print()\n",
    "\n",
    "print(\"Column 3: A[:, 3]\")\n",
    "print(A[:, 3])\n",
    "print()\n",
    "\n",
    "print(\"Submatrix A[1:3, 2:5]:\")\n",
    "print(A[1:3, 2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced indexing: negative indices, steps, fancy indexing\n",
    "\n",
    "print(\"Last row: A[-1, :]\")\n",
    "print(A[-1, :])\n",
    "print()\n",
    "\n",
    "print(\"Every other column: A[:, ::2]\")\n",
    "print(A[:, ::2])\n",
    "print()\n",
    "\n",
    "print(\"Reversed rows: A[::-1, :]\")\n",
    "print(A[::-1, :])\n",
    "print()\n",
    "\n",
    "# Fancy indexing: select specific rows\n",
    "print(\"Rows 0 and 3: A[[0, 3], :]\")\n",
    "print(A[[0, 3], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing\n",
    "\n",
    "print(\"Original:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Boolean mask\n",
    "mask = A > 10\n",
    "print(\"Mask (A > 10):\")\n",
    "print(mask)\n",
    "print()\n",
    "\n",
    "# Apply mask — returns 1D array of matching elements\n",
    "print(\"A[A > 10]:\", A[A > 10])\n",
    "print()\n",
    "\n",
    "# Modify in-place using boolean indexing\n",
    "A_copy = A.copy()\n",
    "A_copy[A_copy > 10] = -1\n",
    "print(\"After A[A > 10] = -1:\")\n",
    "print(A_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different indexing operations\n",
    "\n",
    "A = np.arange(20).reshape(4, 5)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "def show_selection(ax, title, selected_mask):\n",
    "    \"\"\"Highlight selected elements in blue.\"\"\"\n",
    "    display = np.zeros_like(A, dtype=float)\n",
    "    display[selected_mask] = 1\n",
    "    ax.imshow(display, cmap='Blues', vmin=0, vmax=1)\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            color = 'white' if selected_mask[i, j] else 'black'\n",
    "            ax.text(j, i, f'{A[i,j]}', ha='center', va='center', \n",
    "                   fontsize=12, color=color, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_yticks(range(4))\n",
    "\n",
    "# Different selections\n",
    "mask1 = np.zeros((4, 5), dtype=bool); mask1[1, 2] = True\n",
    "show_selection(axes[0, 0], 'A[1, 2] — Single element', mask1)\n",
    "\n",
    "mask2 = np.zeros((4, 5), dtype=bool); mask2[2, :] = True\n",
    "show_selection(axes[0, 1], 'A[2, :] — Row 2', mask2)\n",
    "\n",
    "mask3 = np.zeros((4, 5), dtype=bool); mask3[:, 3] = True\n",
    "show_selection(axes[0, 2], 'A[:, 3] — Column 3', mask3)\n",
    "\n",
    "mask4 = np.zeros((4, 5), dtype=bool); mask4[1:3, 2:5] = True\n",
    "show_selection(axes[1, 0], 'A[1:3, 2:5] — Submatrix', mask4)\n",
    "\n",
    "mask5 = np.zeros((4, 5), dtype=bool); mask5[::2, ::2] = True\n",
    "show_selection(axes[1, 1], 'A[::2, ::2] — Every other', mask5)\n",
    "\n",
    "mask6 = A > 10\n",
    "show_selection(axes[1, 2], 'A[A > 10] — Boolean mask', mask6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak\n",
    "\n",
    "### Views vs. Copies\n",
    "\n",
    "**Critical distinction**:\n",
    "- **Basic slicing** (`A[1:3, :]`) returns a **view** — shares memory with original\n",
    "- **Fancy indexing** (`A[[1, 2], :]`) returns a **copy** — independent memory\n",
    "\n",
    "```python\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "view = A[0, :]      # View: modifying view changes A\n",
    "view[0] = 99        # A is now [[99, 2], [3, 4]]\n",
    "\n",
    "copy = A[[0], :]    # Copy: independent\n",
    "copy[0, 0] = 100    # A is unchanged\n",
    "```\n",
    "\n",
    "**Why it matters**: Views are memory-efficient but can cause subtle bugs if you modify data unexpectedly.\n",
    "\n",
    "### Time Complexity\n",
    "\n",
    "| Operation | Complexity | Returns |\n",
    "|-----------|------------|--------|\n",
    "| Basic slice `A[i:j]` | O(1) | View |\n",
    "| Boolean index `A[mask]` | O(n) | Copy |\n",
    "| Fancy index `A[indices]` | O(k) | Copy |\n",
    "\n",
    "### PyTorch Differences\n",
    "\n",
    "In PyTorch, basic indexing also returns views, but there are additional considerations for autograd:\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x[0]  # View, gradient flows through\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Views vs copies demonstration\n",
    "\n",
    "A = np.arange(6).reshape(2, 3)\n",
    "print(\"Original A:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Basic slice → view\n",
    "view = A[0, :]\n",
    "print(f\"view = A[0, :]: {view}\")\n",
    "print(f\"Shares memory: {np.shares_memory(A, view)}\")\n",
    "\n",
    "view[0] = 99\n",
    "print(f\"After view[0] = 99, A becomes:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Reset\n",
    "A = np.arange(6).reshape(2, 3)\n",
    "\n",
    "# Fancy index → copy\n",
    "copy = A[[0], :]\n",
    "print(f\"copy = A[[0], :]: {copy}\")\n",
    "print(f\"Shares memory: {np.shares_memory(A, copy)}\")\n",
    "\n",
    "copy[0, 0] = 99\n",
    "print(f\"After copy[0, 0] = 99, A is unchanged:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "### Slice Notation\n",
    "\n",
    "For $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the slice $\\mathbf{A}_{i_1:i_2, j_1:j_2}$ extracts:\n",
    "\n",
    "$$\\mathbf{B} \\in \\mathbb{R}^{(i_2-i_1) \\times (j_2-j_1)}$$\n",
    "\n",
    "where $B_{kl} = A_{(i_1+k), (j_1+l)}$ for $k \\in [0, i_2-i_1)$ and $l \\in [0, j_2-j_1)$.\n",
    "\n",
    "### Boolean Indexing as Projection\n",
    "\n",
    "Boolean indexing with mask $\\mathbf{m} \\in \\{0, 1\\}^n$ on vector $\\mathbf{x} \\in \\mathbb{R}^n$ is:\n",
    "\n",
    "$$\\mathbf{x}[\\mathbf{m}] = \\mathbf{P}_\\mathbf{m} \\mathbf{x}$$\n",
    "\n",
    "where $\\mathbf{P}_\\mathbf{m} \\in \\mathbb{R}^{k \\times n}$ is a selection matrix with $k = \\sum_i m_i$ rows, each containing a single 1 in the position of a True value in $\\mathbf{m}$.\n",
    "\n",
    "### Diagonal Extraction\n",
    "\n",
    "For $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, the diagonal is:\n",
    "\n",
    "$$\\text{diag}(\\mathbf{A}) = [A_{00}, A_{11}, \\ldots, A_{n-1,n-1}]^T$$\n",
    "\n",
    "Equivalently: $[\\text{diag}(\\mathbf{A})]_i = A_{ii}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Autograd\n",
    "\n",
    "## Layer 1: Intuition — Automatic calculus\n",
    "\n",
    "Imagine you're hiking on a foggy mountain and you want to reach the valley (lowest point). You can't see far, but you can feel which way is downhill under your feet. That's what **gradients** tell us: the direction of steepest change.\n",
    "\n",
    "Computing gradients by hand is tedious and error-prone. **Autograd** does it automatically by:\n",
    "\n",
    "1. **Recording** every operation you do (building a computation graph)\n",
    "2. **Replaying** backwards to compute gradients via chain rule\n",
    "\n",
    "Think of it like a tape recorder:\n",
    "- Forward: Record `x → x² → x² + 3x → ...`\n",
    "- Backward: Replay in reverse, computing derivatives at each step\n",
    "\n",
    "**Key insight**: You never write derivative code. Just write the forward computation, and PyTorch figures out the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple autograd example\n",
    "\n",
    "# Create a tensor and tell PyTorch to track gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Do some computation\n",
    "y = x**2 + 3*x + 1\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = x² + 3x + 1 = {y.item()}\")\n",
    "\n",
    "# Compute gradient: dy/dx\n",
    "y.backward()\n",
    "\n",
    "print(f\"dy/dx = 2x + 3 = {x.grad.item()}\")\n",
    "print(f\"(At x=2: 2*2 + 3 = 7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate gradients\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# f(x, y) = x²y + xy²\n",
    "f = x**2 * y + x * y**2\n",
    "\n",
    "print(f\"f(1, 2) = 1²·2 + 1·2² = {f.item()}\")\n",
    "\n",
    "# Compute gradients\n",
    "f.backward()\n",
    "\n",
    "print(f\"∂f/∂x = 2xy + y² = 2·1·2 + 2² = {x.grad.item()}\")\n",
    "print(f\"∂f/∂y = x² + 2xy = 1² + 2·1·2 = {y.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient as direction of steepest ascent\n",
    "\n",
    "# Create a function: f(x, y) = sin(x) + cos(y)\n",
    "def f(x, y):\n",
    "    return torch.sin(x) + torch.cos(y)\n",
    "\n",
    "# Compute gradient at several points\n",
    "points = [(-1.0, 0.5), (0.5, 1.0), (1.5, -0.5), (-0.5, -1.0)]\n",
    "gradients = []\n",
    "\n",
    "for px, py in points:\n",
    "    x = torch.tensor(px, requires_grad=True)\n",
    "    y = torch.tensor(py, requires_grad=True)\n",
    "    z = f(x, y)\n",
    "    z.backward()\n",
    "    gradients.append((x.grad.item(), y.grad.item()))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Surface contours\n",
    "xx = np.linspace(-2, 2, 100)\n",
    "yy = np.linspace(-2, 2, 100)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = np.sin(XX) + np.cos(YY)\n",
    "\n",
    "contour = ax.contourf(XX, YY, ZZ, levels=20, cmap='viridis', alpha=0.8)\n",
    "plt.colorbar(contour, label='f(x, y)')\n",
    "\n",
    "# Gradient arrows (pointing uphill)\n",
    "for (px, py), (gx, gy) in zip(points, gradients):\n",
    "    ax.scatter(px, py, color='red', s=100, zorder=5)\n",
    "    ax.quiver(px, py, gx, gy, color='red', scale=5, width=0.01,\n",
    "              label='Gradient (uphill)' if px == points[0][0] else '')\n",
    "    ax.quiver(px, py, -gx, -gy, color='white', scale=5, width=0.01,\n",
    "              label='Negative gradient (downhill)' if px == points[0][0] else '')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Gradients point in direction of steepest ascent\\n(White arrows show descent direction)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The computation graph\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Each operation creates a node in the graph\n",
    "a = x * x      # a = x²\n",
    "b = 3 * x      # b = 3x  \n",
    "c = a + b      # c = x² + 3x\n",
    "y = c + 1      # y = x² + 3x + 1\n",
    "\n",
    "# Print the graph structure\n",
    "print(f\"y = {y}\")\n",
    "print(f\"y.grad_fn = {y.grad_fn}\")\n",
    "print(f\"Graph: y ← {y.grad_fn.next_functions}\")\n",
    "\n",
    "# Backward pass traverses this graph\n",
    "y.backward()\n",
    "print(f\"\\ndy/dx = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak\n",
    "\n",
    "### Computation Graphs\n",
    "\n",
    "PyTorch builds a **Directed Acyclic Graph (DAG)** during forward computation:\n",
    "- **Nodes**: Tensors\n",
    "- **Edges**: Operations (functions)\n",
    "\n",
    "The `grad_fn` attribute stores the backward function for each tensor.\n",
    "\n",
    "### Backward Pass Algorithm\n",
    "\n",
    "```python\n",
    "def backward(loss):\n",
    "    # Initialize: gradient of loss w.r.t. itself is 1\n",
    "    grad_outputs = {loss: 1.0}\n",
    "    \n",
    "    # Traverse graph in reverse topological order\n",
    "    for node in reverse_topological_sort(graph):\n",
    "        grad_output = grad_outputs[node]\n",
    "        \n",
    "        # Apply chain rule via grad_fn\n",
    "        for input_node, grad_input in node.grad_fn(grad_output):\n",
    "            grad_outputs[input_node] += grad_input  # Accumulate\n",
    "    \n",
    "    # Store leaf gradients in .grad\n",
    "    for leaf in leaves:\n",
    "        leaf.grad = grad_outputs[leaf]\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| `requires_grad=True` | Tensor participates in gradient computation |\n",
    "| `grad_fn` | Function to compute gradient (set by operations) |\n",
    "| `backward()` | Compute all gradients via backpropagation |\n",
    "| `.grad` | Accumulated gradient (on leaf tensors) |\n",
    "| `torch.no_grad()` | Context manager to disable gradient tracking |\n",
    "| `detach()` | Create tensor that doesn't track gradients |\n",
    "\n",
    "### Memory Considerations\n",
    "\n",
    "- The computation graph is stored in memory during forward pass\n",
    "- `backward()` frees the graph by default (set `retain_graph=True` to keep)\n",
    "- Use `torch.no_grad()` for inference to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important autograd patterns\n",
    "\n",
    "# 1. Gradients accumulate! Always zero them.\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "for i in range(3):\n",
    "    y = x * x\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i}: x.grad = {x.grad}\")\n",
    "\n",
    "print(\"\\n(Notice gradients accumulate — this is a feature, not a bug!)\")\n",
    "print(\"In training loops, always call optimizer.zero_grad()\\n\")\n",
    "\n",
    "# 2. Zero gradients manually\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "for i in range(3):\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    y = x * x\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i} (after zeroing): x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. no_grad() for inference\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# With gradient tracking (training)\n",
    "y = x * x\n",
    "print(f\"y requires grad: {y.requires_grad}\")\n",
    "print(f\"y has grad_fn: {y.grad_fn is not None}\")\n",
    "\n",
    "# Without gradient tracking (inference)\n",
    "with torch.no_grad():\n",
    "    y_no_grad = x * x\n",
    "    print(f\"\\ny_no_grad requires grad: {y_no_grad.requires_grad}\")\n",
    "    print(f\"y_no_grad has grad_fn: {y_no_grad.grad_fn is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "### Chain Rule\n",
    "\n",
    "For composed functions $y = f(g(x))$:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "For a computation graph with $y = f_n(f_{n-1}(\\cdots f_1(x)))$:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{\\partial f_n}{\\partial f_{n-1}} \\cdot \\frac{\\partial f_{n-1}}{\\partial f_{n-2}} \\cdots \\frac{\\partial f_1}{\\partial x}$$\n",
    "\n",
    "### Multivariate Chain Rule\n",
    "\n",
    "For $y = f(\\mathbf{u})$ where $\\mathbf{u} = g(\\mathbf{x})$:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\sum_j \\frac{\\partial y}{\\partial u_j} \\cdot \\frac{\\partial u_j}{\\partial x_i}$$\n",
    "\n",
    "Or in matrix form:\n",
    "\n",
    "$$\\nabla_\\mathbf{x} y = \\mathbf{J}_g^T \\nabla_\\mathbf{u} y$$\n",
    "\n",
    "where $\\mathbf{J}_g$ is the Jacobian matrix of $g$.\n",
    "\n",
    "### Gradient of Scalar Loss\n",
    "\n",
    "In deep learning, we typically have a scalar loss $L = L(\\mathbf{\\theta})$. The gradient is:\n",
    "\n",
    "$$\\nabla_\\theta L = \\left[ \\frac{\\partial L}{\\partial \\theta_1}, \\frac{\\partial L}{\\partial \\theta_2}, \\ldots, \\frac{\\partial L}{\\partial \\theta_n} \\right]^T$$\n",
    "\n",
    "This is what `loss.backward()` computes and stores in `param.grad` for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: GPU Basics\n",
    "\n",
    "## Layer 1: Intuition — Parallel workers\n",
    "\n",
    "Your CPU is like a brilliant professor who can do one thing at a time, very well.\n",
    "\n",
    "Your GPU is like a classroom of 1000 students — each can only do simple math, but they all work simultaneously.\n",
    "\n",
    "For matrix operations (where we do the same thing to many numbers), the GPU wins:\n",
    "- Multiply two 1000×1000 matrices: ~1 billion operations\n",
    "- CPU: Do them one-by-one (slow)\n",
    "- GPU: Do thousands in parallel (fast)\n",
    "\n",
    "**Key insight**: GPUs are fast for parallel operations (matrix math), but slow for sequential logic and data transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Code + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "\n",
    "print(\"Available devices:\")\n",
    "print(f\"  CPU: Always available\")\n",
    "print(f\"  CUDA (NVIDIA): {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"    Device name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  MPS (Apple Silicon): {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Select best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors between devices\n",
    "\n",
    "# Create tensor on CPU (default)\n",
    "x_cpu = torch.randn(3, 3)\n",
    "print(f\"x_cpu device: {x_cpu.device}\")\n",
    "\n",
    "# Move to GPU/MPS\n",
    "x_gpu = x_cpu.to(device)\n",
    "print(f\"x_gpu device: {x_gpu.device}\")\n",
    "\n",
    "# Move back to CPU (needed for numpy, plotting, etc.)\n",
    "x_back = x_gpu.cpu()\n",
    "print(f\"x_back device: {x_back.device}\")\n",
    "\n",
    "# Create directly on device\n",
    "y = torch.randn(3, 3, device=device)\n",
    "print(f\"y device (created directly): {y.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: CPU vs GPU\n",
    "import time\n",
    "\n",
    "sizes = [100, 500, 1000, 2000]\n",
    "cpu_times = []\n",
    "gpu_times = []\n",
    "\n",
    "for size in sizes:\n",
    "    # CPU timing\n",
    "    a_cpu = torch.randn(size, size)\n",
    "    b_cpu = torch.randn(size, size)\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        c_cpu = a_cpu @ b_cpu\n",
    "    cpu_times.append((time.time() - start) / 10)\n",
    "    \n",
    "    # GPU timing (if available)\n",
    "    if device.type != 'cpu':\n",
    "        a_gpu = a_cpu.to(device)\n",
    "        b_gpu = b_cpu.to(device)\n",
    "        \n",
    "        # Warmup\n",
    "        _ = a_gpu @ b_gpu\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            c_gpu = a_gpu @ b_gpu\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        gpu_times.append((time.time() - start) / 10)\n",
    "    else:\n",
    "        gpu_times.append(None)\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Size':>6} | {'CPU (ms)':>10} | {'GPU (ms)':>10} | {'Speedup':>8}\")\n",
    "print(\"-\" * 45)\n",
    "for size, cpu_t, gpu_t in zip(sizes, cpu_times, gpu_times):\n",
    "    if gpu_t:\n",
    "        speedup = cpu_t / gpu_t\n",
    "        print(f\"{size:>6} | {cpu_t*1000:>10.2f} | {gpu_t*1000:>10.2f} | {speedup:>7.1f}x\")\n",
    "    else:\n",
    "        print(f\"{size:>6} | {cpu_t*1000:>10.2f} | {'N/A':>10} | {'N/A':>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: CS Speak\n",
    "\n",
    "### Device Management\n",
    "\n",
    "```python\n",
    "# Recommended pattern for device-agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model and data to same device\n",
    "model = MyModel().to(device)\n",
    "data = data.to(device)\n",
    "output = model(data)\n",
    "```\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Device mismatch**: Operations require tensors on the same device\n",
    "   ```python\n",
    "   x_cpu = torch.randn(3)\n",
    "   y_gpu = torch.randn(3, device='cuda')\n",
    "   z = x_cpu + y_gpu  # ERROR!\n",
    "   ```\n",
    "\n",
    "2. **Forgetting to sync**: GPU operations are asynchronous\n",
    "   ```python\n",
    "   torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "   ```\n",
    "\n",
    "3. **Data transfer overhead**: Moving data to GPU is slow\n",
    "   - Batch data transfers\n",
    "   - Keep data on GPU during training\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "```python\n",
    "# Check GPU memory\n",
    "torch.cuda.memory_allocated()  # Currently allocated\n",
    "torch.cuda.memory_reserved()   # Reserved by caching allocator\n",
    "\n",
    "# Free cached memory\n",
    "torch.cuda.empty_cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Mathematical Formalism\n",
    "\n",
    "### Why GPUs Excel at Matrix Operations\n",
    "\n",
    "Matrix multiplication $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$ for $\\mathbf{A} \\in \\mathbb{R}^{m \\times k}$, $\\mathbf{B} \\in \\mathbb{R}^{k \\times n}$:\n",
    "\n",
    "$$C_{ij} = \\sum_{l=1}^{k} A_{il} B_{lj}$$\n",
    "\n",
    "This requires $m \\cdot n \\cdot k$ multiply-add operations.\n",
    "\n",
    "**Key observation**: Each $C_{ij}$ can be computed **independently**. This is \"embarrassingly parallel.\"\n",
    "\n",
    "- **CPU**: Compute $C_{ij}$ entries sequentially\n",
    "- **GPU**: Compute thousands of $C_{ij}$ entries simultaneously\n",
    "\n",
    "### Computational Intensity\n",
    "\n",
    "The ratio of compute to memory access determines GPU suitability:\n",
    "\n",
    "$$\\text{Arithmetic Intensity} = \\frac{\\text{FLOPs}}{\\text{Bytes transferred}}$$\n",
    "\n",
    "For matrix multiplication: $O(n^3)$ FLOPs, $O(n^2)$ memory → High intensity → GPU-friendly\n",
    "\n",
    "For element-wise operations: $O(n)$ FLOPs, $O(n)$ memory → Low intensity → Often CPU-bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Now it's your turn. Complete these exercises to solidify your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Tensor Surgery\n",
    "\n",
    "Create a 10×10 matrix filled with ones, then set the diagonal to zeros using indexing (not loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Tensor Surgery\n",
    "\n",
    "def diagonal_to_zero(n):\n",
    "    \"\"\"\n",
    "    Create an n×n matrix of ones with zeros on the diagonal.\n",
    "    \n",
    "    Args:\n",
    "        n: Size of the square matrix\n",
    "    \n",
    "    Returns:\n",
    "        n×n numpy array with 1s everywhere except 0s on diagonal\n",
    "    \"\"\"\n",
    "    # TODO: Create matrix of ones\n",
    "    # TODO: Set diagonal to zeros (hint: np.diag_indices or np.fill_diagonal)\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "result = diagonal_to_zero(10)\n",
    "print(result)\n",
    "print(f\"\\nDiagonal values: {np.diag(result)}\")\n",
    "print(f\"Sum of matrix (should be 90): {result.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Broadcasting Challenge\n",
    "\n",
    "Normalize each row of a matrix to sum to 1 (i.e., convert each row to a probability distribution).\n",
    "\n",
    "**Hint**: Use `keepdims=True` in your sum to maintain shape for broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Broadcasting Challenge\n",
    "\n",
    "def normalize_rows(matrix):\n",
    "    \"\"\"\n",
    "    Normalize each row to sum to 1.\n",
    "    \n",
    "    Args:\n",
    "        matrix: 2D numpy array\n",
    "    \n",
    "    Returns:\n",
    "        Normalized matrix where each row sums to 1\n",
    "    \"\"\"\n",
    "    # TODO: Compute row sums with keepdims=True\n",
    "    # TODO: Divide matrix by row sums (broadcasting!)\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_matrix = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [10, 20, 30, 40],\n",
    "    [5, 5, 5, 5]\n",
    "])\n",
    "\n",
    "normalized = normalize_rows(test_matrix)\n",
    "print(\"Original:\")\n",
    "print(test_matrix)\n",
    "print(\"\\nNormalized (each row sums to 1):\")\n",
    "print(normalized)\n",
    "print(f\"\\nRow sums: {normalized.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradient Computation\n",
    "\n",
    "Compute the gradient of $f(x, y) = x^2 y + \\sin(xy)$ at the point $(1, \\pi)$ using PyTorch autograd.\n",
    "\n",
    "**Expected answer**:\n",
    "- $\\frac{\\partial f}{\\partial x} = 2xy + y\\cos(xy)$ → at $(1, \\pi)$: $2\\pi + \\pi\\cos(\\pi) = 2\\pi - \\pi = \\pi \\approx 3.14$\n",
    "- $\\frac{\\partial f}{\\partial y} = x^2 + x\\cos(xy)$ → at $(1, \\pi)$: $1 + \\cos(\\pi) = 1 - 1 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Gradient Computation\n",
    "\n",
    "def compute_gradient():\n",
    "    \"\"\"\n",
    "    Compute gradient of f(x,y) = x²y + sin(xy) at (1, π).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (df/dx, df/dy) at (1, π)\n",
    "    \"\"\"\n",
    "    # TODO: Create tensors for x=1 and y=π with requires_grad=True\n",
    "    # TODO: Compute f(x, y)\n",
    "    # TODO: Call backward()\n",
    "    # TODO: Return gradients\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "grad_x, grad_y = compute_gradient()\n",
    "print(f\"∂f/∂x at (1, π) = {grad_x:.4f}\")\n",
    "print(f\"∂f/∂y at (1, π) = {grad_y:.4f}\")\n",
    "print(f\"\\nExpected: ∂f/∂x ≈ π ≈ {np.pi:.4f}, ∂f/∂y ≈ 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Why This Matters\n",
    "\n",
    "Everything we've covered is foundational for the rest of this curriculum:\n",
    "\n",
    "| Concept | Where it appears later |\n",
    "|---------|------------------------|\n",
    "| **Tensors** | Embeddings (Week 2) are vectors; attention matrices are 4D tensors |\n",
    "| **Broadcasting** | Normalization in transformers; batch operations everywhere |\n",
    "| **Indexing** | Gathering embeddings; masking in attention |\n",
    "| **Autograd** | Training any model; computing Jacobians (Week 5-6) |\n",
    "| **GPU** | Practical necessity for any non-trivial model |\n",
    "\n",
    "The gradient computation you just did by hand? That's exactly what happens trillions of times when training GPT-4. Autograd makes it tractable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "**Official Documentation**:\n",
    "- [PyTorch Tensors Tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "- [NumPy Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "- [PyTorch Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "**Deep Dives**:\n",
    "- [Jay Alammar: A Visual Intro to NumPy](https://jalammar.github.io/visual-numpy/)\n",
    "- [PyTorch Internals: Autograd](http://blog.ezyang.com/2019/05/pytorch-internals/)\n",
    "\n",
    "**Wiki**:\n",
    "- [Glossary](../../wiki/glossary.md) — tensor, broadcasting, autograd\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection Questions\n",
    "\n",
    "Before moving on, consider:\n",
    "\n",
    "1. **Why do we need autograd instead of computing gradients by hand?**\n",
    "   - Think about a neural network with millions of parameters...\n",
    "\n",
    "2. **When would broadcasting cause unexpected behavior?**\n",
    "   - What if you accidentally have a (3,) vector and a (3, 1) vector?\n",
    "\n",
    "3. **Why is GPU memory transfer a bottleneck?**\n",
    "   - What does this imply for how we should structure data pipelines?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Up\n",
    "\n",
    "In **01b: Linear Algebra Refresh**, we'll revisit:\n",
    "- Vectors as arrows, matrices as transformations\n",
    "- Eigenvalues and eigenvectors (crucial for understanding dynamics)\n",
    "- SVD for dimensionality reduction\n",
    "\n",
    "All using the tensor tools you just learned.\n",
    "\n",
    "→ [01b: Linear Algebra Refresh](01b-linear-algebra-refresh.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
