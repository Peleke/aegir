{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0.1: Taste Demo \u2014 Salience Scoring \u2014 Core\n",
    "\n",
    "**Arc 0: Probabilistic Foundations** | Module 1 of 8\n",
    "\n",
    "**Prerequisites**: None\n",
    "\n",
    "**Time**: ~60-90 minutes\n",
    "\n",
    "**Implementation target**: buildlog `SalienceScorer` \u2014 replaces substring matching for rule compliance evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Explain why substring matching fails for evaluating agent rule compliance\n",
    "- [ ] Decompose rule compliance into linguistic, structural, and outcome signals\n",
    "- [ ] Implement a `SalienceScorer` with configurable, updatable weights\n",
    "- [ ] State the falsifiable claim for your scorer and test it against intuitive ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# INTRO\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "*[Hero image placeholder \u2014 a detective inspecting code through a magnifying glass,\n",
    "but the magnifying glass shows a probability distribution instead of the code]*\n",
    "\n",
    "## The Setup\n",
    "\n",
    "Last Tuesday, the bandit demoted your best rule.\n",
    "\n",
    "An agent followed \"always define interfaces before implementations\" perfectly --\n",
    "wrote a clean `Protocol`, then a concrete class implementing it. The bandit\n",
    "checked `if rule_text in agent_output`, got `False` (the agent didn't *quote*\n",
    "the rule, it *followed* the rule), and logged a negative reward.\n",
    "\n",
    "Your best rule is being punished for working.\n",
    "\n",
    "That's not evaluation. That's string matching cosplay. Let's build something\n",
    "that actually measures whether an agent followed a rule.\n",
    "\n",
    "**By the end of this notebook**, you'll have a working `SalienceScorer` that\n",
    "evaluates rule compliance through three signals \u2014 linguistic, structural, and\n",
    "outcome \u2014 and can explain its scores in plain English.\n",
    "\n",
    "Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# LAYER 0: THE PROBLEM\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "Here are two rules from a real buildlog CLAUDE.md:\n",
    "\n",
    "- *\"Always define interfaces before implementations\"*\n",
    "- *\"Always validate parsed dates are within valid ranges\"*\n",
    "\n",
    "And here's agent output from a coding session:\n",
    "\n",
    "```python\n",
    "class PaymentProcessor:\n",
    "    def process(self, amount: float) -> bool:\n",
    "        if amount <= 0:\n",
    "            raise ValueError(\"Amount must be positive\")\n",
    "        return self._charge(amount)\n",
    "```\n",
    "\n",
    "The `contains` check says: neither rule was followed (the text doesn't appear).\n",
    "\n",
    "A human reviewer says: the interface rule is irrelevant here (no interface needed\n",
    "for a standalone processor), and the date rule is irrelevant (no dates involved).\n",
    "The agent did fine.\n",
    "\n",
    "The `contains` check is giving the bandit *wrong reward signals*. The bandit is\n",
    "learning from noise. This is worse than useless \u2014 it's actively harmful.\n",
    "\n",
    "**Three failure modes of substring matching:**\n",
    "\n",
    "1. **False negatives**: Agent follows the rule's *intent* without quoting it verbatim\n",
    "2. **False positives**: Agent mentions the rule text without actually following it\n",
    "3. **Irrelevance blindness**: Can't distinguish \"rule violated\" from \"rule doesn't apply\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# LAYER 1: INTUITION\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "Think about how a human code reviewer evaluates compliance. They check three things:\n",
    "\n",
    "1. **Does the output speak the rule's language?** (Linguistic signal)\n",
    "   - If the rule says \"define interfaces before implementations\" and the code has\n",
    "     `Protocol`, `ABC`, `abstractmethod` \u2014 that's a vocabulary match.\n",
    "\n",
    "2. **Does the output follow the rule's prescribed pattern?** (Structural signal)\n",
    "   - Is there an abstract class defined *before* the concrete class?\n",
    "\n",
    "3. **Did the task succeed?** (Outcome signal)\n",
    "   - A rule can't be \"followed\" if the thing it was supposed to help with broke.\n",
    "\n",
    "Our scorer combines these three signals with weights:\n",
    "\n",
    "```\n",
    "S = w_l * linguistic + w_s * structural + w_o * outcome\n",
    "```\n",
    "\n",
    "This is a weighted linear combination. It's the simplest model that could work.\n",
    "It's definitely wrong in interesting ways (we'll fix that later).\n",
    "But it's already dramatically better than `contains`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# LAYER 2: CODE + VIZ\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "## The Road Map\n",
    "\n",
    "```\n",
    "Problem 1 (mock data)  -->  Show contains fails\n",
    "    |\n",
    "Problem 2 (linguistic signal)  -->  Uses ENTRIES from P1\n",
    "    |\n",
    "Problem 3 (structural signal)  -->  Uses ENTRIES + extract_code_blocks from P2\n",
    "    |\n",
    "Problem 4 (outcome signal)  -->  Uses ENTRIES from P1\n",
    "    |\n",
    "Problem 5 (SalienceScorer)  -->  Assembles signals from P2+P3+P4\n",
    "    |\n",
    "Problem 6 (validate)  -->  Runs scorer from P5 against ENTRIES, tests falsifiable claim\n",
    "```\n",
    "\n",
    "Each problem consumes the previous problem's output. Don't skip ahead.\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Problem 1: Set Up Mock Buildlog Entries and Show How `contains` Fails\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Let's create realistic mock data. Each entry has a rule, agent output, task\n",
    "outcome, and your intuitive compliance rating (0-1) as ground truth.\n",
    "\n",
    "We need 10 entries covering:\n",
    "- Perfect compliance\n",
    "- Rule doesn't apply (irrelevance)\n",
    "- Rule violated\n",
    "- False positive (agent quotes rule without following it)\n",
    "- Failed tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BuildlogEntry:\n",
    "    \"\"\"A single buildlog entry with a rule, agent output, and ground truth.\"\"\"\n",
    "    rule: str\n",
    "    task: str\n",
    "    agent_output: str\n",
    "    task_succeeded: bool\n",
    "    intuitive_compliance: float  # Your expert rating, 0-1\n",
    "\n",
    "\n",
    "ENTRIES = [\n",
    "    # Entry 0: Perfect compliance -- interface defined before implementation\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Create a storage backend for user profiles\",\n",
    "        agent_output=\"\"\"I'll start with the interface:\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class StorageBackend(ABC):\n",
    "    @abstractmethod\n",
    "    def save(self, key: str, data: dict) -> None: ...\n",
    "    @abstractmethod\n",
    "    def load(self, key: str) -> dict: ...\n",
    "\n",
    "class PostgresBackend(StorageBackend):\n",
    "    def save(self, key, data):\n",
    "        self.conn.execute(\"INSERT INTO profiles ...\", data)\n",
    "    def load(self, key):\n",
    "        return self.conn.execute(\"SELECT ...\", key).fetchone()\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.95,\n",
    "    ),\n",
    "    # Entry 1: Rule doesn't apply -- no interface needed\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Write a utility function to slugify strings\",\n",
    "        agent_output=\"\"\"```python\n",
    "import re\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^\\\\w\\\\s-]', '', text)\n",
    "    return re.sub(r'[-\\\\s]+', '-', text)\n",
    "```\n",
    "Simple utility -- no interface needed here.\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.5,\n",
    "    ),\n",
    "    # Entry 2: Rule violated -- no interface, but one was needed\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Create a payment processor with multiple gateways\",\n",
    "        agent_output=\"\"\"```python\n",
    "class StripeProcessor:\n",
    "    def charge(self, amount): ...\n",
    "\n",
    "class PayPalProcessor:\n",
    "    def charge(self, amount): ...\n",
    "```\n",
    "Both processors handle charges directly.\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.15,\n",
    "    ),\n",
    "    # Entry 3: FALSE POSITIVE -- agent quotes the rule but doesn't follow it\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Build a notification service with email and SMS\",\n",
    "        agent_output=\"\"\"Per the rule 'always define interfaces before implementations',\n",
    "I'll build the notification service:\n",
    "```python\n",
    "class EmailNotifier:\n",
    "    def send(self, to, msg): ...\n",
    "\n",
    "class SMSNotifier:\n",
    "    def send(self, to, msg): ...\n",
    "```\n",
    "Both handle notifications.\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.10,  # Quoted the rule but didn't follow it!\n",
    "    ),\n",
    "    # Entry 4: Perfect date validation\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always validate parsed dates are within valid ranges\",\n",
    "        task=\"Parse user-submitted event dates\",\n",
    "        agent_output=\"\"\"```python\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_event_date(raw: str) -> datetime:\n",
    "    dt = datetime.fromisoformat(raw)\n",
    "    if dt.year < 2020 or dt.year > 2030:\n",
    "        raise ValueError(f\"Date {dt} outside valid range 2020-2030\")\n",
    "    return dt\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.90,\n",
    "    ),\n",
    "    # Entry 5: Date parsing without validation\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always validate parsed dates are within valid ranges\",\n",
    "        task=\"Import CSV with timestamps\",\n",
    "        agent_output=\"\"\"```python\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def import_csv(path):\n",
    "    with open(path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            ts = datetime.fromisoformat(row[3])\n",
    "            yield {\"name\": row[0], \"timestamp\": ts}\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.20,\n",
    "    ),\n",
    "    # Entry 6: No dates involved at all\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always validate parsed dates are within valid ranges\",\n",
    "        task=\"Implement retry logic for HTTP requests\",\n",
    "        agent_output=\"\"\"```python\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "def retry(url, max_retries=3):\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            return httpx.get(url)\n",
    "        except httpx.TimeoutException:\n",
    "            time.sleep(2 ** i)\n",
    "    raise RuntimeError(f\"Failed after {max_retries} retries\")\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.5,\n",
    "    ),\n",
    "    # Entry 7: FAILED TASK -- interface defined but code is broken\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Create a caching layer with Redis and in-memory backends\",\n",
    "        agent_output=\"\"\"```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CacheBackend(ABC):\n",
    "    @abstractmethod\n",
    "    def get(self, key: str) -> str: ...\n",
    "\n",
    "class RedisCache(CacheBackend):\n",
    "    def get(self, key):\n",
    "        return self.client.get(key)  # self.client is never initialized\n",
    "```\n",
    "The tests fail -- RedisCache crashes on instantiation.\"\"\",\n",
    "        task_succeeded=False,\n",
    "        intuitive_compliance=0.35,  # Good structure but broken code\n",
    "    ),\n",
    "    # Entry 8: FAILED TASK -- date validation present but wrong logic\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always validate parsed dates are within valid ranges\",\n",
    "        task=\"Build a booking system date parser\",\n",
    "        agent_output=\"\"\"```python\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_booking_date(raw: str) -> datetime:\n",
    "    dt = datetime.fromisoformat(raw)\n",
    "    if dt.year < 2020:\n",
    "        raise ValueError(\"Too old\")\n",
    "    # BUG: no upper bound check, accepts year 9999\n",
    "    return dt\n",
    "```\n",
    "Booking system crashed on a test with year 2099.\"\"\",\n",
    "        task_succeeded=False,\n",
    "        intuitive_compliance=0.30,  # Partial validation, task failed\n",
    "    ),\n",
    "    # Entry 9: Perfect compliance on interface rule, complex case\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Design a plugin system for buildlog extractors\",\n",
    "        agent_output=\"\"\"I'll define the extractor protocol first:\n",
    "```python\n",
    "from typing import Protocol\n",
    "\n",
    "class ExtractorProtocol(Protocol):\n",
    "    def extract(self, text: str) -> list[str]: ...\n",
    "    def confidence(self) -> float: ...\n",
    "\n",
    "class RegexExtractor:\n",
    "    def extract(self, text):\n",
    "        return re.findall(r'RULE: (.+)', text)\n",
    "    def confidence(self):\n",
    "        return 0.6\n",
    "\n",
    "class LLMExtractor:\n",
    "    def extract(self, text):\n",
    "        return self.client.extract_rules(text)\n",
    "    def confidence(self):\n",
    "        return 0.8\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.95,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(ENTRIES)} mock buildlog entries.\")\n",
    "print(f\"Rules: {set(e.rule for e in ENTRIES)}\")\n",
    "print(f\"Failed tasks: {sum(1 for e in ENTRIES if not e.task_succeeded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how `contains` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_check(rule: str, output: str) -> float:\n",
    "    \"\"\"The current approach: does the rule text appear in the output?\"\"\"\n",
    "    return 1.0 if rule.lower() in output.lower() else 0.0\n",
    "\n",
    "\n",
    "print(\"Contains check results vs intuitive ratings:\\n\")\n",
    "print(f\"{'Entry':>7}  {'Contains':>9}  {'Intuitive':>10}  {'Delta':>6}\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "total_error = 0\n",
    "for i, e in enumerate(ENTRIES):\n",
    "    c = contains_check(e.rule, e.agent_output)\n",
    "    delta = abs(c - e.intuitive_compliance)\n",
    "    total_error += delta\n",
    "    marker = \" <<<\" if delta > 0.4 else \"\"\n",
    "    print(f\"  Entry {i}:  {c:>8.2f}  {e.intuitive_compliance:>10.2f}  {delta:>5.2f}{marker}\")\n",
    "\n",
    "print(f\"\\nMean absolute error: {total_error / len(ENTRIES):.2f}\")\n",
    "print(\"<<< = error > 0.4 (bandit is learning wrong from these)\")\n",
    "print(\"\\nNotice Entry 3: contains returns 1.0 (agent quoted the rule) but\")\n",
    "print(\"intuitive is 0.10 (agent didn't actually follow it). That's a false positive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`contains` gets it wrong in both directions. Entries 0, 4, 9 are false negatives\n",
    "(agent followed the rule, `contains` says no). Entry 3 is a false positive\n",
    "(agent quoted the rule but didn't follow it, `contains` says yes).\n",
    "\n",
    "Let's fix this one signal at a time. Each problem below builds on the previous one.\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Problem 2: Linguistic Signal Detection\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Using `ENTRIES` from Problem 1, let's build the first signal.\n",
    "\n",
    "The linguistic signal asks: does the output use vocabulary associated with the rule?\n",
    "\n",
    "Your task:\n",
    "1. Implement `linguistic_signal(rule, output)` returning float in [0, 1]\n",
    "2. Extract keywords from the rule (words > 3 chars)\n",
    "3. Check for each keyword (or synonyms) in the output\n",
    "4. Weight code occurrences at 1.0, prose at 0.5\n",
    "\n",
    "You'll need `extract_code_blocks` (provided) to separate code from prose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "SYNONYM_MAP = {\n",
    "    \"interface\": [\"interface\", \"protocol\", \"abc\", \"abstract\"],\n",
    "    \"interfaces\": [\"interface\", \"protocol\", \"abc\", \"abstract\"],\n",
    "    \"implementations\": [\"implementation\", \"concrete\", \"class\"],\n",
    "    \"define\": [\"define\", \"create\", \"class\"],\n",
    "    \"validate\": [\"validate\", \"check\", \"verify\", \"assert\", \"raise\"],\n",
    "    \"parsed\": [\"parsed\", \"parse\", \"fromisoformat\", \"strptime\"],\n",
    "    \"dates\": [\"date\", \"datetime\", \"timestamp\"],\n",
    "    \"valid\": [\"valid\", \"range\", \"between\", \"boundary\"],\n",
    "    \"ranges\": [\"range\", \"between\", \"min\", \"max\", \"limit\"],\n",
    "    \"always\": [\"always\"],\n",
    "    \"within\": [\"within\", \"inside\", \"between\"],\n",
    "    \"before\": [\"before\", \"first\", \"prior\"],\n",
    "}\n",
    "\n",
    "\n",
    "def extract_code_blocks(text: str) -> tuple[str, str]:\n",
    "    \"\"\"Split text into (code, prose) by extracting ```...``` blocks.\"\"\"\n",
    "    code_blocks = re.findall(r'```(?:python)?\\n(.*?)```', text, re.DOTALL)\n",
    "    code = '\\n'.join(code_blocks)\n",
    "    prose = re.sub(r'```(?:python)?\\n.*?```', '', text, flags=re.DOTALL)\n",
    "    return code, prose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_signal(rule: str, output: str) -> float:\n",
    "    \"\"\"\n",
    "    Measure how much of the rule's key vocabulary appears in the output.\n",
    "    Code mentions weighted 1.0, prose mentions weighted 0.5.\n",
    "    Returns float in [0, 1].\n",
    "    \"\"\"\n",
    "    # --- YOUR CODE BELOW ---\n",
    "    pass\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# \u2502 def linguistic_signal(rule: str, output: str) -> float:\n",
    "# \u2502     keywords = [w.lower() for w in rule.split() if len(w) > 3]\n",
    "# \u2502     if not keywords:\n",
    "# \u2502         return 0.0\n",
    "# \u2502     code, prose = extract_code_blocks(output)\n",
    "# \u2502     code_lower, prose_lower = code.lower(), prose.lower()\n",
    "# \u2502     scores = []\n",
    "# \u2502     for kw in keywords:\n",
    "# \u2502         synonyms = SYNONYM_MAP.get(kw, [kw])\n",
    "# \u2502         in_code = any(s in code_lower for s in synonyms)\n",
    "# \u2502         in_prose = any(s in prose_lower for s in synonyms)\n",
    "# \u2502         if in_code:\n",
    "# \u2502             scores.append(1.0)\n",
    "# \u2502         elif in_prose:\n",
    "# \u2502             scores.append(0.5)\n",
    "# \u2502         else:\n",
    "# \u2502             scores.append(0.0)\n",
    "# \u2502     return min(1.0, sum(scores) / len(scores))\n",
    "# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: linguistic signal should score Entry 0 (perfect compliance) high\n",
    "# and Entry 6 (no dates, irrelevant rule) low.\n",
    "_l0 = linguistic_signal(ENTRIES[0].rule, ENTRIES[0].agent_output)\n",
    "_l6 = linguistic_signal(ENTRIES[6].rule, ENTRIES[6].agent_output)\n",
    "assert _l0 is not None, \"Did you implement linguistic_signal? It returned None.\"\n",
    "assert isinstance(_l0, float), f\"Expected float, got {type(_l0)}\"\n",
    "assert _l0 > 0.5, f\"Entry 0 (perfect compliance) should score > 0.5, got {_l0:.2f}\"\n",
    "assert _l6 < _l0, f\"Entry 6 (irrelevant rule) should score lower than Entry 0\"\n",
    "\n",
    "print(\"Linguistic signal scores:\")\n",
    "for i, e in enumerate(ENTRIES):\n",
    "    score = linguistic_signal(e.rule, e.agent_output)\n",
    "    print(f\"  Entry {i}: {score:.2f}  (intuitive: {e.intuitive_compliance:.2f})\")\n",
    "\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good -- `linguistic_signal` picks up vocabulary matches. But it can't tell\n",
    "structure from word soup. Entry 3 (false positive -- quoted rule, didn't follow\n",
    "it) might score high linguistically. We need the structural signal.\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Problem 3: Structural Signal Detection\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Now that we have `linguistic_signal` and `extract_code_blocks`, we need the\n",
    "second signal. The structural signal asks: does the output's *structure* match\n",
    "the rule's prescribed pattern? This is rule-specific.\n",
    "\n",
    "Your task:\n",
    "1. Implement `check_interface_before_impl(output)` \u2014 score in [0, 1]\n",
    "2. Implement `check_date_validation(output)` \u2014 score in [0, 1]\n",
    "3. Wire them up via `structural_signal(rule, output)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interface_before_impl(output: str) -> float:\n",
    "    \"\"\"\n",
    "    1.0 = clear interface-first pattern\n",
    "    0.5 = interface exists but order unclear\n",
    "    0.25 = no interface but agent acknowledged rule doesn't apply\n",
    "    0.0 = no interface pattern detected\n",
    "    \"\"\"\n",
    "    # --- YOUR CODE BELOW ---\n",
    "    pass\n",
    "\n",
    "\n",
    "def check_date_validation(output: str) -> float:\n",
    "    \"\"\"\n",
    "    1.0 = date parsing with explicit range validation\n",
    "    0.5 = some validation but incomplete\n",
    "    0.25 = dates present but no validation\n",
    "    0.0 = no date handling detected\n",
    "    \"\"\"\n",
    "    # --- YOUR CODE BELOW ---\n",
    "    pass\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# \u2502 def check_interface_before_impl(output: str) -> float:\n",
    "# \u2502     code, prose = extract_code_blocks(output)\n",
    "# \u2502     iface_pattern = re.compile(\n",
    "# \u2502         r'class\\s+\\w+\\(\\s*(?:ABC|Protocol)\\s*\\)|@abstractmethod|class\\s+\\w+\\(Protocol\\)'\n",
    "# \u2502     )\n",
    "# \u2502     has_abstract = bool(iface_pattern.search(code))\n",
    "# \u2502     has_concrete = bool(re.search(r'class\\s+\\w+', code))\n",
    "# \u2502     if has_abstract and has_concrete:\n",
    "# \u2502         abstract_match = iface_pattern.search(code)\n",
    "# \u2502         for m in re.finditer(r'class\\s+\\w+', code):\n",
    "# \u2502             if m.start() != abstract_match.start() and m.start() > abstract_match.start():\n",
    "# \u2502                 return 1.0\n",
    "# \u2502         return 0.5\n",
    "# \u2502     acknowledged = bool(re.search(\n",
    "# \u2502         r'(no.*interface.*needed|utility|simple|no interface needed)',\n",
    "# \u2502         prose, re.I\n",
    "# \u2502     ))\n",
    "# \u2502     if acknowledged:\n",
    "# \u2502         return 0.25\n",
    "# \u2502     return 0.0\n",
    "# \u2502\n",
    "# \u2502 def check_date_validation(output: str) -> float:\n",
    "# \u2502     code, _ = extract_code_blocks(output)\n",
    "# \u2502     has_date = bool(re.search(r'(datetime|fromisoformat|strptime)', code, re.I))\n",
    "# \u2502     has_upper_and_lower = bool(re.search(\n",
    "# \u2502         r'if.*(<|>).*\\d.*(<|>)', code, re.DOTALL\n",
    "# \u2502     )) or bool(re.search(r'raise.*ValueError', code, re.I))\n",
    "# \u2502     has_any_check = bool(re.search(\n",
    "# \u2502         r'(if.*(<|>|<=|>=).*\\d|raise.*ValueError)', code, re.I\n",
    "# \u2502     ))\n",
    "# \u2502     if has_date and has_upper_and_lower:\n",
    "# \u2502         return 1.0\n",
    "# \u2502     if has_date and has_any_check:\n",
    "# \u2502         return 0.5\n",
    "# \u2502     if has_date:\n",
    "# \u2502         return 0.25\n",
    "# \u2502     return 0.0\n",
    "# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "\n",
    "STRUCTURAL_PATTERNS: dict[str, Callable[[str], float]] = {\n",
    "    \"Always define interfaces before implementations\": check_interface_before_impl,\n",
    "    \"Always validate parsed dates are within valid ranges\": check_date_validation,\n",
    "}\n",
    "\n",
    "\n",
    "def structural_signal(rule: str, output: str) -> float:\n",
    "    \"\"\"Dispatch to the appropriate structural checker.\"\"\"\n",
    "    checker = STRUCTURAL_PATTERNS.get(rule)\n",
    "    return checker(output) if checker else 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: Entry 0 (perfect interface) should score high,\n",
    "# Entry 3 (quoted rule, no interface) should score low.\n",
    "_s0 = structural_signal(ENTRIES[0].rule, ENTRIES[0].agent_output)\n",
    "_s3 = structural_signal(ENTRIES[3].rule, ENTRIES[3].agent_output)\n",
    "assert _s0 is not None, \"Did you implement the structural checkers? Got None.\"\n",
    "assert _s0 >= 0.75, f\"Entry 0 (perfect interface) should score >= 0.75, got {_s0:.2f}\"\n",
    "assert _s3 < 0.5, f\"Entry 3 (false positive) should score < 0.5, got {_s3:.2f}\"\n",
    "\n",
    "print(\"Structural signal scores:\")\n",
    "for i, e in enumerate(ENTRIES):\n",
    "    score = structural_signal(e.rule, e.agent_output)\n",
    "    print(f\"  Entry {i}: {score:.2f}  (intuitive: {e.intuitive_compliance:.2f})\")\n",
    "\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can catch the false positive: Entry 3 has high linguistic signal (mentions\n",
    "the right words) but low structural signal (no actual interface). Two signals\n",
    "down, one to go.\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Problem 4: Outcome Signal\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "The simplest signal: did the task succeed? This is binary for now, but it\n",
    "matters. Entries 7 and 8 have `task_succeeded=False` \u2014 no matter how good the\n",
    "structure looks, a crashing implementation isn't compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_signal(entry: BuildlogEntry) -> float:\n",
    "    \"\"\"1.0 if task succeeded, 0.0 otherwise.\"\"\"\n",
    "    # --- YOUR CODE BELOW ---\n",
    "    pass\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# \u2502 def outcome_signal(entry: BuildlogEntry) -> float:\n",
    "# \u2502     return 1.0 if entry.task_succeeded else 0.0\n",
    "# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "# Verify\n",
    "_o0 = outcome_signal(ENTRIES[0])\n",
    "_o7 = outcome_signal(ENTRIES[7])\n",
    "assert _o0 is not None, \"Did you implement outcome_signal? Got None.\"\n",
    "assert _o0 == 1.0, f\"Entry 0 (succeeded) should be 1.0, got {_o0}\"\n",
    "assert _o7 == 0.0, f\"Entry 7 (failed) should be 0.0, got {_o7}\"\n",
    "\n",
    "print(\"Outcome signal: 8 succeeded, 2 failed.\")\n",
    "print(\"Entries 7 and 8 will get penalized regardless of linguistic/structural scores.\")\n",
    "\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three signals ready. Let's assemble them.\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Problem 5: Assemble the SalienceScorer\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Combine `linguistic_signal`, `structural_signal`, and `outcome_signal` from\n",
    "Problems 2-4 into a single class.\n",
    "\n",
    "Your task:\n",
    "1. Implement `SalienceScorer` with configurable weights\n",
    "2. Weights must be updatable (constitutional rule: not hardcoded forever)\n",
    "3. `score` returns a `SalienceResult` with component breakdown\n",
    "4. `explain` returns plain English (constitutional rule: explain 0.7)\n",
    "5. State the **falsifiable claim**: what would make this scorer wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SalienceResult:\n",
    "    \"\"\"Result with component breakdown.\"\"\"\n",
    "    score: float\n",
    "    linguistic: float\n",
    "    structural: float\n",
    "    outcome: float\n",
    "    weights: dict\n",
    "\n",
    "    def explain(self) -> str:\n",
    "        \"\"\"Plain-English explanation.\"\"\"\n",
    "        # --- YOUR CODE BELOW ---\n",
    "        pass\n",
    "\n",
    "\n",
    "class SalienceScorer:\n",
    "    \"\"\"\n",
    "    Scores agent output for rule compliance using three signals.\n",
    "\n",
    "    Falsifiable claim: Rankings agree with expert intuitive ratings\n",
    "    (Spearman rho > 0.8). If not, recalibrate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w_linguistic=0.4, w_structural=0.4, w_outcome=0.2):\n",
    "        # --- YOUR CODE BELOW ---\n",
    "        pass\n",
    "\n",
    "    def update_weights(self, w_l: float, w_s: float, w_o: float) -> None:\n",
    "        \"\"\"Update weights. Must sum to 1.\"\"\"\n",
    "        # --- YOUR CODE BELOW ---\n",
    "        pass\n",
    "\n",
    "    def score(self, entry: BuildlogEntry) -> SalienceResult:\n",
    "        \"\"\"Score a single buildlog entry.\"\"\"\n",
    "        # --- YOUR CODE BELOW ---\n",
    "        pass\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# \u2502 @dataclass\n",
    "# \u2502 class SalienceResult:\n",
    "# \u2502     score: float\n",
    "# \u2502     linguistic: float\n",
    "# \u2502     structural: float\n",
    "# \u2502     outcome: float\n",
    "# \u2502     weights: dict\n",
    "# \u2502\n",
    "# \u2502     def explain(self) -> str:\n",
    "# \u2502         level = (\n",
    "# \u2502             \"strong\" if self.score >= 0.75\n",
    "# \u2502             else \"moderate\" if self.score >= 0.5\n",
    "# \u2502             else \"weak\" if self.score >= 0.25\n",
    "# \u2502             else \"minimal\"\n",
    "# \u2502         )\n",
    "# \u2502         parts = []\n",
    "# \u2502         if self.linguistic >= 0.7:\n",
    "# \u2502             parts.append(\"uses relevant vocabulary\")\n",
    "# \u2502         elif self.linguistic >= 0.4:\n",
    "# \u2502             parts.append(\"some vocabulary overlap\")\n",
    "# \u2502         else:\n",
    "# \u2502             parts.append(\"little vocabulary match\")\n",
    "# \u2502         if self.structural >= 0.7:\n",
    "# \u2502             parts.append(\"follows the prescribed pattern\")\n",
    "# \u2502         elif self.structural >= 0.4:\n",
    "# \u2502             parts.append(\"partially follows the pattern\")\n",
    "# \u2502         else:\n",
    "# \u2502             parts.append(\"doesn't follow the pattern\")\n",
    "# \u2502         parts.append(\"task succeeded\" if self.outcome >= 0.5 else \"task failed\")\n",
    "# \u2502         return f\"Score {self.score:.2f} -- {level} compliance. The output {', '.join(parts)}.\"\n",
    "# \u2502\n",
    "# \u2502 class SalienceScorer:\n",
    "# \u2502     FALSIFIABLE_CLAIM = \"Rankings agree with expert ratings (Spearman rho > 0.8).\"\n",
    "# \u2502\n",
    "# \u2502     def __init__(self, w_linguistic=0.4, w_structural=0.4, w_outcome=0.2):\n",
    "# \u2502         self.update_weights(w_linguistic, w_structural, w_outcome)\n",
    "# \u2502\n",
    "# \u2502     def update_weights(self, w_l, w_s, w_o):\n",
    "# \u2502         total = w_l + w_s + w_o\n",
    "# \u2502         assert abs(total - 1.0) < 1e-6, f\"Weights must sum to 1, got {total}\"\n",
    "# \u2502         self.weights = {\"linguistic\": w_l, \"structural\": w_s, \"outcome\": w_o}\n",
    "# \u2502\n",
    "# \u2502     def score(self, entry):\n",
    "# \u2502         l = linguistic_signal(entry.rule, entry.agent_output)\n",
    "# \u2502         s = structural_signal(entry.rule, entry.agent_output)\n",
    "# \u2502         o = outcome_signal(entry)\n",
    "# \u2502         combined = self.weights[\"linguistic\"]*l + self.weights[\"structural\"]*s + self.weights[\"outcome\"]*o\n",
    "# \u2502         return SalienceResult(round(combined, 4), round(l, 4), round(s, 4), round(o, 4), dict(self.weights))\n",
    "# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: scorer should be constructible and produce results\n",
    "scorer = SalienceScorer(w_linguistic=0.4, w_structural=0.4, w_outcome=0.2)\n",
    "r0 = scorer.score(ENTRIES[0])\n",
    "assert r0 is not None, \"scorer.score() returned None. Did you implement it?\"\n",
    "assert hasattr(r0, 'explain'), \"SalienceResult needs an explain() method.\"\n",
    "assert r0.score > 0.5, f\"Entry 0 (perfect compliance) should score > 0.5, got {r0.score}\"\n",
    "print(f\"Entry 0 score: {r0.score:.2f}\")\n",
    "print(f\"Explanation: {r0.explain()}\")\n",
    "\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 6: Run the Scorer, Visualize, Test the Falsifiable Claim\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Now run the `scorer` from Problem 5 against all `ENTRIES` from Problem 1.\n",
    "\n",
    "Your task:\n",
    "1. Score all 10 entries\n",
    "2. Create a side-by-side bar chart: salience score vs. intuitive rating\n",
    "3. Compute Spearman rank correlation -- does it beat 0.8?\n",
    "4. Print the explanation for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "# Score all entries, plot comparison, compute Spearman correlation\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# \u2502 results = [scorer.score(e) for e in ENTRIES]\n",
    "# \u2502 salience_scores = [r.score for r in results]\n",
    "# \u2502 intuitive = [e.intuitive_compliance for e in ENTRIES]\n",
    "# \u2502\n",
    "# \u2502 rho, pval = scipy_stats.spearmanr(salience_scores, intuitive)\n",
    "# \u2502 print(f\"Spearman rho: {rho:.3f}  (p={pval:.4f})\")\n",
    "# \u2502 print(f\"Falsifiable claim threshold: rho > 0.8\")\n",
    "# \u2502 print(f\"Result: {'PASS' if rho > 0.8 else 'NEEDS RECALIBRATION'}\")\n",
    "# \u2502 print()\n",
    "# \u2502\n",
    "# \u2502 x = np.arange(len(ENTRIES))\n",
    "# \u2502 width = 0.35\n",
    "# \u2502 fig, ax = plt.subplots(figsize=(14, 5))\n",
    "# \u2502 bars1 = ax.bar(x - width/2, salience_scores, width, label='Salience Score', color='#2196F3')\n",
    "# \u2502 bars2 = ax.bar(x + width/2, intuitive, width, label='Intuitive Rating', color='#FF9800')\n",
    "# \u2502 # Mark failed tasks\n",
    "# \u2502 for i, e in enumerate(ENTRIES):\n",
    "# \u2502     if not e.task_succeeded:\n",
    "# \u2502         ax.annotate('FAILED', (i, max(salience_scores[i], intuitive[i]) + 0.05),\n",
    "# \u2502                     ha='center', fontsize=8, color='red')\n",
    "# \u2502 ax.set_xlabel('Entry')\n",
    "# \u2502 ax.set_ylabel('Score')\n",
    "# \u2502 ax.set_title('SalienceScorer vs. Intuitive Ratings')\n",
    "# \u2502 ax.set_xticks(x)\n",
    "# \u2502 ax.legend()\n",
    "# \u2502 ax.set_ylim(0, 1.2)\n",
    "# \u2502 plt.tight_layout()\n",
    "# \u2502 plt.show()\n",
    "# \u2502\n",
    "# \u2502 print(\"\\nExplanations:\")\n",
    "# \u2502 for i, (e, r) in enumerate(zip(ENTRIES, results)):\n",
    "# \u2502     status = \"OK\" if e.task_succeeded else \"FAILED\"\n",
    "# \u2502     print(f\"\\n  Entry {i} [{status}]: {e.task[:50]}\")\n",
    "# \u2502     print(f\"    {r.explain()}\")\n",
    "# \u2502     print(f\"    Components: L={r.linguistic:.2f} S={r.structural:.2f} O={r.outcome:.2f}\")\n",
    "# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Backfill: Why a Weighted Linear Combination?\n",
    "\n",
    "We made two assumptions that are both wrong and both useful:\n",
    "\n",
    "1. **Linearity**: Signals combine additively. In reality, high structural + high\n",
    "   linguistic is stronger evidence than either alone (interaction effects).\n",
    "   But linear is the simplest baseline.\n",
    "\n",
    "2. **Independence**: The three signals are independent. They're not -- vocabulary\n",
    "   overlap correlates with structural compliance. But treating them as independent\n",
    "   lets us build and test each detector separately.\n",
    "\n",
    "Both assumptions break down. That's fine. Module 0.2 builds the probability\n",
    "foundations to handle more sophisticated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# EXERCISES\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "## Exercise 1: Break the Scorer\n",
    "\n",
    "Add 3 new `BuildlogEntry` instances to `ENTRIES` that expose failure modes of\n",
    "the `SalienceScorer`. For each entry, explain:\n",
    "- What the scorer gets wrong (predicted score vs. your intuitive rating)\n",
    "- Why it fails (which signal is misleading)\n",
    "- How you'd fix it (what signal or logic would handle this case)\n",
    "\n",
    "**Success criteria**: At least one entry where the scorer's ranking disagrees\n",
    "with intuition by > 0.3.\n",
    "\n",
    "## Exercise 2: Weight Tuning\n",
    "\n",
    "Try different weight configurations on the 10 entries:\n",
    "- `(0.7, 0.2, 0.1)` \u2014 linguistic-heavy\n",
    "- `(0.2, 0.7, 0.1)` \u2014 structural-heavy\n",
    "- `(0.33, 0.33, 0.34)` \u2014 uniform\n",
    "\n",
    "**Success criteria**: Report the Spearman rho for each configuration. Identify\n",
    "which gives the best correlation. If any configuration achieves rho > 0.9,\n",
    "explain whether that's likely overfitting to 10 data points (hint: it probably is).\n",
    "\n",
    "## Exercise 3 [PUBLISH]: Write the \"Contains Check Takedown\"\n",
    "\n",
    "Write 500-800 words explaining why substring matching fails for evaluating\n",
    "agent rule compliance. Target: practitioners building AI agent systems.\n",
    "\n",
    "Structure:\n",
    "1. The problem: you have rules, you need to evaluate compliance\n",
    "2. The naive approach: `contains` / substring matching\n",
    "3. Three failure modes (use examples from this notebook)\n",
    "4. The alternative: decompose into linguistic + structural + outcome signals\n",
    "5. Why this matters for bandit-based rule learning systems\n",
    "\n",
    "Draft for: *\"Here's what everyone gets wrong about evaluating AI agent output\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 workspace\n",
    "draft = \"\"\"\n",
    "# Here's What Everyone Gets Wrong About Evaluating AI Agent Output\n",
    "\n",
    "TODO: Write your draft here.\n",
    "\"\"\"\n",
    "print(draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# OUTRO\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "## What Just Happened\n",
    "\n",
    "You built a salience scorer that replaces blind substring matching with three\n",
    "targeted signals:\n",
    "\n",
    "- **Linguistic**: Does the output speak the rule's language?\n",
    "- **Structural**: Does the output follow the rule's prescribed pattern?\n",
    "- **Outcome**: Did the task succeed?\n",
    "\n",
    "Combined with configurable weights (`S = w_l * linguistic + w_s * structural +\n",
    "w_o * outcome`), this gives the bandit a dramatically better reward signal.\n",
    "\n",
    "You also established three constitutional rules for the entire arc:\n",
    "\n",
    "1. Every scoring function must have a falsifiable claim\n",
    "2. Weights must be updatable from data, not hardcoded forever\n",
    "3. If you can't explain what a score of 0.7 means in plain English, the scorer isn't ready\n",
    "\n",
    "## Publication Note\n",
    "\n",
    "Exercise 3 is a draft for *\"Here's what everyone gets wrong about evaluating\n",
    "AI agent output.\"* Run an edit pass and it's ready to publish.\n",
    "\n",
    "## What's Next\n",
    "\n",
    "The weights are hand-picked. Why 0.4/0.4/0.2? Because it felt right. That's\n",
    "not science. In Module 0.2, we build the probability foundations to make this\n",
    "rigorous \u2014 so the weights can be learned from data instead of vibes.\n",
    "\n",
    "--> [Module 0.2: Probability & Counting](../module-0.2-probability-counting/0.2-probability-counting-core.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# RESOURCES\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "- **Salience (cognitive science)**: Salience as attention-weighted relevance comes\n",
    "  from cognitive/perceptual psychology. Our scorer approximates what a human\n",
    "  reviewer does intuitively.\n",
    "- **Weighted linear combinations**: Any introductory linear algebra text covers\n",
    "  why this is the simplest useful model. We'll formalize this in Arc 1.\n",
    "- **From the archive**: `archive/v1-week-based/notebooks/` \u2014 earlier explorations\n",
    "  of the bandit that this module's scorer feeds into.\n",
    "- **Blitzstein & Hwang, *Introduction to Probability***: Ch. 1-2 set up the\n",
    "  probability framework we'll use starting in Module 0.2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}