{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0.1: Taste Demo — Salience Scoring — Core\n",
    "\n",
    "**Arc 0: Probabilistic Foundations** | Module 1 of 8\n",
    "\n",
    "**Prerequisites**: None\n",
    "\n",
    "**Time**: ~60-90 minutes\n",
    "\n",
    "**Implementation target**: buildlog `SalienceScorer` — replaces substring matching for rule compliance evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Explain why substring matching fails for evaluating agent rule compliance\n",
    "- [ ] Decompose rule compliance into linguistic, structural, and outcome signals\n",
    "- [ ] Implement a `SalienceScorer` with configurable, updatable weights\n",
    "- [ ] State the falsifiable claim for your scorer and test it against intuitive ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# INTRO\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## The Setup\n",
    "\n",
    "While ..., you had one agent, one rule, and\n",
    "one evaluation function. The rule was simple: *\"Always define interfaces before\n",
    "implementations.\"* The agent followed it perfectly — wrote a clean `Protocol`,\n",
    "then a concrete class implementing it.\n",
    "\n",
    "The evaluation function ran `if rule_text in agent_output`, got `False` (the\n",
    "agent didn't *quote* the rule, it *followed* the rule), and logged a negative\n",
    "reward.\n",
    "\n",
    "The agent was punished for doing its job.\n",
    "\n",
    "**By the end of this notebook**, you'll have a working `SalienceScorer` that\n",
    "evaluates rule compliance through three signals — and can explain its scores\n",
    "in plain English.\n",
    "\n",
    "Let's see exactly what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the rule, here's what the agent produced, and here's what the check said.\n",
    "\n",
    "one_rule = \"Always define interfaces before implementations\"\n",
    "\n",
    "one_output = \"\"\"I'll start with the interface:\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class StorageBackend(ABC):\n",
    "    @abstractmethod\n",
    "    def save(self, key: str, data: dict) -> None: ...\n",
    "    @abstractmethod\n",
    "    def load(self, key: str) -> dict: ...\n",
    "\n",
    "class PostgresBackend(StorageBackend):\n",
    "    def save(self, key, data):\n",
    "        self.conn.execute(\"INSERT INTO profiles ...\", data)\n",
    "    def load(self, key):\n",
    "        return self.conn.execute(\"SELECT ...\", key).fetchone()\n",
    "```\"\"\"\n",
    "\n",
    "def contains_check(rule: str, output: str) -> float:\n",
    "    \"\"\"The current approach: does the rule text appear in the output?\"\"\"\n",
    "    return 1.0 if rule.lower() in output.lower() else 0.0\n",
    "\n",
    "score = contains_check(one_rule, one_output)\n",
    "print(f\"Rule:  {one_rule}\")\n",
    "print(f\"Score: {score}\")\n",
    "print()\n",
    "print(\"The agent wrote a Protocol, then a concrete class. Perfect compliance.\")\n",
    "print(f\"The check returned {score}. It looked for the literal string and didn't find it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent followed the rule. The check said it didn't.\n",
    "\n",
    "How would *you* evaluate this? Read that agent output again. What tells you\n",
    "the rule was followed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Intuition\n",
    "\n",
    "What words jump out from the agent output? `ABC`, `abstractmethod`, `Protocol`,\n",
    "`StorageBackend` — these are the *vocabulary* of the rule \"define interfaces\n",
    "before implementations.\" The agent didn't quote the rule; it spoke its language.\n",
    "\n",
    "What if we could check for that? Not the exact rule text, but the rule's\n",
    "*vocabulary* showing up in the output.\n",
    "\n",
    "Let's try it on this one example first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Code - Do NOT Edit\n",
    "SYNONYM_MAP = {\n",
    "    \"interface\": [\"interface\", \"protocol\", \"abc\", \"abstract\"],\n",
    "    \"interfaces\": [\"interface\", \"protocol\", \"abc\", \"abstract\"],\n",
    "    \"implementations\": [\"implementation\", \"concrete\", \"class\"],\n",
    "    \"define\": [\"define\", \"create\", \"class\"],\n",
    "    \"validate\": [\"validate\", \"check\", \"verify\", \"assert\", \"raise\"],\n",
    "    \"parsed\": [\"parsed\", \"parse\", \"fromisoformat\", \"strptime\"],\n",
    "    \"dates\": [\"date\", \"datetime\", \"timestamp\"],\n",
    "    \"valid\": [\"valid\", \"range\", \"between\", \"boundary\"],\n",
    "    \"ranges\": [\"range\", \"between\", \"min\", \"max\", \"limit\"],\n",
    "    \"always\": [\"always\"],\n",
    "    \"within\": [\"within\", \"inside\", \"between\"],\n",
    "    \"before\": [\"before\", \"first\", \"prior\"],\n",
    "}\n",
    "\n",
    "\n",
    "def extract_code_blocks(text: str) -> tuple[str, str]:\n",
    "    \"\"\"Split text into (code, prose) by extracting ```...``` blocks.\"\"\"\n",
    "    code_blocks = re.findall(r'```(?:python)?\\n(.*?)```', text, re.DOTALL)\n",
    "    code = '\\n'.join(code_blocks)\n",
    "    prose = re.sub(r'```(?:python)?\\n.*?```', '', text, flags=re.DOTALL)\n",
    "    return code, prose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 2: Score Vocabulary Overlap Between Rule and Output\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "The linguistic signal asks: does the output use vocabulary associated with the rule?\n",
    "\n",
    "Your task:\n",
    "1. Implement `linguistic_signal(rule, output)` returning float in [0, 1]\n",
    "2. Extract keywords from the rule (words > 3 chars)\n",
    "3. Check for each keyword (or synonyms) in the output\n",
    "4. Weight code occurrences at 1.0, prose at 0.5\n",
    "\n",
    "You'll need `extract_code_blocks` (provided above) to separate code from prose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_signal(rule: str, output: str) -> float:\n",
    "    \"\"\"\n",
    "    Measure how much of the rule's key vocabulary appears in the output.\n",
    "    Code mentions weighted 1.0, prose mentions weighted 0.5.\n",
    "    Returns float in [0, 1].\n",
    "    \"\"\"\n",
    "    # --- YOUR CODE BELOW ---\n",
    "    pass\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ def linguistic_signal(rule: str, output: str) -> float:\n",
    "# │     keywords = [w.lower() for w in rule.split() if len(w) > 3]\n",
    "# │     if not keywords:\n",
    "# │         return 0.0\n",
    "# │     code, prose = extract_code_blocks(output)\n",
    "# │     code_lower, prose_lower = code.lower(), prose.lower()\n",
    "# │     scores = []\n",
    "# │     for kw in keywords:\n",
    "# │         synonyms = SYNONYM_MAP.get(kw, [kw])\n",
    "# │         in_code = any(s in code_lower for s in synonyms)\n",
    "# │         in_prose = any(s in prose_lower for s in synonyms)\n",
    "# │         if in_code:\n",
    "# │             scores.append(1.0)\n",
    "# │         elif in_prose:\n",
    "# │             scores.append(0.5)\n",
    "# │         else:\n",
    "# │             scores.append(0.0)\n",
    "# │     return min(1.0, sum(scores) / len(scores))\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify on our single example first.\n",
    "_l_single = linguistic_signal(one_rule, one_output)\n",
    "assert _l_single is not None, \"Did you implement linguistic_signal? It returned None.\"\n",
    "assert isinstance(_l_single, float), f\"Expected float, got {type(_l_single)}\"\n",
    "assert _l_single > 0.5, f\"This example has perfect compliance -- should score > 0.5, got {_l_single:.2f}\"\n",
    "\n",
    "print(f\"Linguistic signal on our single example: {_l_single:.2f}\")\n",
    "print(f\"Contains check on the same example:      {contains_check(one_rule, one_output):.2f}\")\n",
    "print()\n",
    "print(\"One example works. But what about edge cases?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary matching catches what `contains` missed. But one example doesn't\n",
    "tell us much. What about these cases?\n",
    "\n",
    "- What if the rule *doesn't apply* to the task? (No interfaces needed)\n",
    "- What if the agent *quotes* the rule but doesn't *follow* it?\n",
    "- What if the code looks right but *crashes*?\n",
    "- What about a completely different rule — like date validation?\n",
    "\n",
    "We need a proper test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 1: Build the Test Suite\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Now that you've felt the problem on one example, let's build the full dataset.\n",
    "Each entry has a rule, agent output, task outcome, and your intuitive compliance\n",
    "rating (0-1) as ground truth.\n",
    "\n",
    "We need 10 entries covering:\n",
    "- Perfect compliance\n",
    "- Rule doesn't apply (irrelevance)\n",
    "- Rule violated\n",
    "- False positive (agent quotes rule without following it)\n",
    "- Failed tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BuildlogEntry:\n",
    "    \"\"\"A single buildlog entry with a rule, agent output, and ground truth.\"\"\"\n",
    "    rule: str\n",
    "    task: str\n",
    "    agent_output: str\n",
    "    task_succeeded: bool\n",
    "    intuitive_compliance: float  # Your expert rating, 0-1\n",
    "\n",
    "\n",
    "ENTRIES = [\n",
    "    # Entry 0: Perfect compliance -- interface defined before implementation\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Create a storage backend for user profiles\",\n",
    "        agent_output=\"\"\"I'll start with the interface:\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class StorageBackend(ABC):\n",
    "    @abstractmethod\n",
    "    def save(self, key: str, data: dict) -> None: ...\n",
    "    @abstractmethod\n",
    "    def load(self, key: str) -> dict: ...\n",
    "\n",
    "class PostgresBackend(StorageBackend):\n",
    "    def save(self, key, data):\n",
    "        self.conn.execute(\"INSERT INTO profiles ...\", data)\n",
    "    def load(self, key):\n",
    "        return self.conn.execute(\"SELECT ...\", key).fetchone()\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.95,\n",
    "    ),\n",
    "    # Entry 1: Rule doesn't apply -- no interface needed\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Write a utility function to slugify strings\",\n",
    "        agent_output=\"\"\"```python\n",
    "import re\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^\\\\w\\\\s-]', '', text)\n",
    "    return re.sub(r'[-\\\\s]+', '-', text)\n",
    "```\n",
    "Simple utility -- no interface needed here.\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.5,\n",
    "    ),\n",
    "    # Entry 2: Rule violated -- no interface, but one was needed\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Create a payment processor with multiple gateways\",\n",
    "        agent_output=\"\"\"```python\n",
    "class StripeProcessor:\n",
    "    def charge(self, amount): ...\n",
    "\n",
    "class PayPalProcessor:\n",
    "    def charge(self, amount): ...\n",
    "```\n",
    "Both processors handle charges directly.\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.15,\n",
    "    ),\n",
    "    # Entry 3: FALSE POSITIVE -- agent quotes the rule but doesn't follow it\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Build a notification service with email and SMS\",\n",
    "        agent_output=\"\"\"Per the rule 'always define interfaces before implementations',\n",
    "I'll build the notification service:\n",
    "```python\n",
    "class EmailNotifier:\n",
    "    def send(self, to, msg): ...\n",
    "\n",
    "class SMSNotifier:\n",
    "    def send(self, to, msg): ...\n",
    "```\n",
    "Both handle notifications.\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.10,  # Quoted the rule but didn't follow it!\n",
    "    ),\n",
    "    # Entry 4: Perfect date validation\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always validate parsed dates are within valid ranges\",\n",
    "        task=\"Parse user-submitted event dates\",\n",
    "        agent_output=\"\"\"```python\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_event_date(raw: str) -> datetime:\n",
    "    dt = datetime.fromisoformat(raw)\n",
    "    if dt.year < 2020 or dt.year > 2030:\n",
    "        raise ValueError(f\"Date {dt} outside valid range 2020-2030\")\n",
    "    return dt\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.90,\n",
    "    ),\n",
    "    # Entry 5: Date parsing without validation\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always validate parsed dates are within valid ranges\",\n",
    "        task=\"Import CSV with timestamps\",\n",
    "        agent_output=\"\"\"```python\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def import_csv(path):\n",
    "    with open(path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            ts = datetime.fromisoformat(row[3])\n",
    "            yield {\"name\": row[0], \"timestamp\": ts}\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.20,\n",
    "    ),\n",
    "    # Entry 6: No dates involved at all\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always validate parsed dates are within valid ranges\",\n",
    "        task=\"Implement retry logic for HTTP requests\",\n",
    "        agent_output=\"\"\"```python\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "def retry(url, max_retries=3):\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            return httpx.get(url)\n",
    "        except httpx.TimeoutException:\n",
    "            time.sleep(2 ** i)\n",
    "    raise RuntimeError(f\"Failed after {max_retries} retries\")\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.5,\n",
    "    ),\n",
    "    # Entry 7: FAILED TASK -- interface defined but code is broken\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Create a caching layer with Redis and in-memory backends\",\n",
    "        agent_output=\"\"\"```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CacheBackend(ABC):\n",
    "    @abstractmethod\n",
    "    def get(self, key: str) -> str: ...\n",
    "\n",
    "class RedisCache(CacheBackend):\n",
    "    def get(self, key):\n",
    "        return self.client.get(key)  # self.client is never initialized\n",
    "```\n",
    "The tests fail -- RedisCache crashes on instantiation.\"\"\",\n",
    "        task_succeeded=False,\n",
    "        intuitive_compliance=0.35,  # Good structure but broken code\n",
    "    ),\n",
    "    # Entry 8: FAILED TASK -- date validation present but wrong logic\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always validate parsed dates are within valid ranges\",\n",
    "        task=\"Build a booking system date parser\",\n",
    "        agent_output=\"\"\"```python\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_booking_date(raw: str) -> datetime:\n",
    "    dt = datetime.fromisoformat(raw)\n",
    "    if dt.year < 2020:\n",
    "        raise ValueError(\"Too old\")\n",
    "    # BUG: no upper bound check, accepts year 9999\n",
    "    return dt\n",
    "```\n",
    "Booking system crashed on a test with year 2099.\"\"\",\n",
    "        task_succeeded=False,\n",
    "        intuitive_compliance=0.30,  # Partial validation, task failed\n",
    "    ),\n",
    "    # Entry 9: Perfect compliance on interface rule, complex case\n",
    "    BuildlogEntry(\n",
    "        rule=\"Always define interfaces before implementations\",\n",
    "        task=\"Design a plugin system for buildlog extractors\",\n",
    "        agent_output=\"\"\"I'll define the extractor protocol first:\n",
    "```python\n",
    "from typing import Protocol\n",
    "\n",
    "class ExtractorProtocol(Protocol):\n",
    "    def extract(self, text: str) -> list[str]: ...\n",
    "    def confidence(self) -> float: ...\n",
    "\n",
    "class RegexExtractor:\n",
    "    def extract(self, text):\n",
    "        return re.findall(r'RULE: (.+)', text)\n",
    "    def confidence(self):\n",
    "        return 0.6\n",
    "\n",
    "class LLMExtractor:\n",
    "    def extract(self, text):\n",
    "        return self.client.extract_rules(text)\n",
    "    def confidence(self):\n",
    "        return 0.8\n",
    "```\"\"\",\n",
    "        task_succeeded=True,\n",
    "        intuitive_compliance=0.95,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(ENTRIES)} mock buildlog entries.\")\n",
    "print(f\"Rules: {set(e.rule for e in ENTRIES)}\")\n",
    "print(f\"Failed tasks: {sum(1 for e in ENTRIES if not e.task_succeeded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run contains_check AND linguistic_signal on all 10, side by side.\n",
    "\n",
    "print(\"Contains vs Linguistic vs Intuitive:\\n\")\n",
    "print(f\"{'Entry':>7}  {'Contains':>9}  {'Linguistic':>11}  {'Intuitive':>10}\")\n",
    "print(\"  \" + \"-\" * 55)\n",
    "for i, e in enumerate(ENTRIES):\n",
    "    c = contains_check(e.rule, e.agent_output)\n",
    "    l = linguistic_signal(e.rule, e.agent_output)\n",
    "    marker = \"\"\n",
    "    if i == 3:\n",
    "        marker = \"  <<< false positive (quoted rule, didn't follow)\"\n",
    "    print(f\"  Entry {i}:  {c:>8.2f}  {l:>10.2f}  {e.intuitive_compliance:>10.2f}{marker}\")\n",
    "\n",
    "print()\n",
    "print(\"Linguistic signal is better than contains across the board.\")\n",
    "print(\"But look at Entry 3 -- it scores HIGH linguistically because the\")\n",
    "print(\"agent mentioned interface-related words. It just didn't build one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words aren't enough. Entry 3 mentions interfaces without building one. The\n",
    "agent *talked about* the rule instead of *following* it, and vocabulary matching\n",
    "can't tell the difference.\n",
    "\n",
    "We need to check *structure* — did the output actually contain the pattern\n",
    "the rule prescribes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 3: Structural Signal Detection\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "The structural signal asks: does the output's *structure* match the rule's\n",
    "prescribed pattern? This is rule-specific.\n",
    "\n",
    "Your task:\n",
    "1. Implement `check_interface_before_impl(output)` — score in [0, 1]\n",
    "2. Implement `check_date_validation(output)` — score in [0, 1]\n",
    "3. Wire them up via `structural_signal(rule, output)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interface_before_impl(output: str) -> float:\n",
    "    \"\"\"\n",
    "    1.0 = clear interface-first pattern\n",
    "    0.5 = interface exists but order unclear\n",
    "    0.25 = no interface but agent acknowledged rule doesn't apply\n",
    "    0.0 = no interface pattern detected\n",
    "    \"\"\"\n",
    "    # --- YOUR CODE BELOW ---\n",
    "    pass\n",
    "\n",
    "\n",
    "def check_date_validation(output: str) -> float:\n",
    "    \"\"\"\n",
    "    1.0 = date parsing with explicit range validation\n",
    "    0.5 = some validation but incomplete\n",
    "    0.25 = dates present but no validation\n",
    "    0.0 = no date handling detected\n",
    "    \"\"\"\n",
    "    # --- YOUR CODE BELOW ---\n",
    "    pass\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ def check_interface_before_impl(output: str) -> float:\n",
    "# │     code, prose = extract_code_blocks(output)\n",
    "# │     iface_pattern = re.compile(\n",
    "# │         r'class\\s+\\w+\\(\\s*(?:ABC|Protocol)\\s*\\)|@abstractmethod|class\\s+\\w+\\(Protocol\\)'\n",
    "# │     )\n",
    "# │     has_abstract = bool(iface_pattern.search(code))\n",
    "# │     has_concrete = bool(re.search(r'class\\s+\\w+', code))\n",
    "# │     if has_abstract and has_concrete:\n",
    "# │         abstract_match = iface_pattern.search(code)\n",
    "# │         for m in re.finditer(r'class\\s+\\w+', code):\n",
    "# │             if m.start() != abstract_match.start() and m.start() > abstract_match.start():\n",
    "# │                 return 1.0\n",
    "# │         return 0.5\n",
    "# │     acknowledged = bool(re.search(\n",
    "# │         r'(no.*interface.*needed|utility|simple|no interface needed)',\n",
    "# │         prose, re.I\n",
    "# │     ))\n",
    "# │     if acknowledged:\n",
    "# │         return 0.25\n",
    "# │     return 0.0\n",
    "# │\n",
    "# │ def check_date_validation(output: str) -> float:\n",
    "# │     code, _ = extract_code_blocks(output)\n",
    "# │     has_date = bool(re.search(r'(datetime|fromisoformat|strptime)', code, re.I))\n",
    "# │     has_upper_and_lower = bool(re.search(\n",
    "# │         r'if.*(<|>).*\\d.*(<|>)', code, re.DOTALL\n",
    "# │     )) or bool(re.search(r'raise.*ValueError', code, re.I))\n",
    "# │     has_any_check = bool(re.search(\n",
    "# │         r'(if.*(<|>|<=|>=).*\\d|raise.*ValueError)', code, re.I\n",
    "# │     ))\n",
    "# │     if has_date and has_upper_and_lower:\n",
    "# │         return 1.0\n",
    "# │     if has_date and has_any_check:\n",
    "# │         return 0.5\n",
    "# │     if has_date:\n",
    "# │         return 0.25\n",
    "# │     return 0.0\n",
    "# └─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "STRUCTURAL_PATTERNS: dict[str, Callable[[str], float]] = {\n",
    "    \"Always define interfaces before implementations\": check_interface_before_impl,\n",
    "    \"Always validate parsed dates are within valid ranges\": check_date_validation,\n",
    "}\n",
    "\n",
    "\n",
    "def structural_signal(rule: str, output: str) -> float:\n",
    "    \"\"\"Dispatch to the appropriate structural checker.\"\"\"\n",
    "    checker = STRUCTURAL_PATTERNS.get(rule)\n",
    "    return checker(output) if checker else 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: Entry 0 (perfect interface) should score high,\n",
    "# Entry 3 (quoted rule, no interface) should score low.\n",
    "_s0 = structural_signal(ENTRIES[0].rule, ENTRIES[0].agent_output)\n",
    "_s3 = structural_signal(ENTRIES[3].rule, ENTRIES[3].agent_output)\n",
    "assert _s0 is not None, \"Did you implement the structural checkers? Got None.\"\n",
    "assert _s0 >= 0.75, f\"Entry 0 (perfect interface) should score >= 0.75, got {_s0:.2f}\"\n",
    "assert _s3 < 0.5, f\"Entry 3 (false positive) should score < 0.5, got {_s3:.2f}\"\n",
    "\n",
    "print(\"Structural signal scores:\")\n",
    "for i, e in enumerate(ENTRIES):\n",
    "    score = structural_signal(e.rule, e.agent_output)\n",
    "    print(f\"  Entry {i}: {score:.2f}  (intuitive: {e.intuitive_compliance:.2f})\")\n",
    "\n",
    "print()\n",
    "print(\"Entry 3 now caught: high linguistic, low structural. False positive detected.\")\n",
    "\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two signals down. Linguistic catches vocabulary, structural catches patterns,\n",
    "and together they caught the false positive.\n",
    "\n",
    "But what about Entries 7 and 8? Entry 7 has perfect structure (interface before\n",
    "implementation) but the code crashes. Entry 8 has partial date validation but\n",
    "the booking system failed. The code *looks* right but *doesn't work*.\n",
    "\n",
    "We need a third signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 4: Outcome Signal\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "The simplest signal: did the task succeed? This is binary for now, but it\n",
    "matters. Entries 7 and 8 have `task_succeeded=False` — no matter how good the\n",
    "structure looks, a crashing implementation isn't compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_signal(entry: BuildlogEntry) -> float:\n",
    "    \"\"\"1.0 if task succeeded, 0.0 otherwise.\"\"\"\n",
    "    # --- YOUR CODE BELOW ---\n",
    "    pass\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ def outcome_signal(entry: BuildlogEntry) -> float:\n",
    "# │     return 1.0 if entry.task_succeeded else 0.0\n",
    "# └─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Verify\n",
    "_o0 = outcome_signal(ENTRIES[0])\n",
    "_o7 = outcome_signal(ENTRIES[7])\n",
    "assert _o0 is not None, \"Did you implement outcome_signal? Got None.\"\n",
    "assert _o0 == 1.0, f\"Entry 0 (succeeded) should be 1.0, got {_o0}\"\n",
    "assert _o7 == 0.0, f\"Entry 7 (failed) should be 0.0, got {_o7}\"\n",
    "\n",
    "print(\"Outcome signal: 8 succeeded, 2 failed.\")\n",
    "print(\"Entries 7 and 8 will get penalized regardless of linguistic/structural scores.\")\n",
    "\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three signals. Three numbers per entry. To collapse them into one score, you\n",
    "need a weighted average — the same thing you've computed since GPA. Give each\n",
    "signal a weight, multiply, sum.\n",
    "\n",
    "That's the whole idea. Let's package it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 5: Assemble the SalienceScorer\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Package `linguistic_signal`, `structural_signal`, and `outcome_signal` from\n",
    "Problems 2-4 into a single class.\n",
    "\n",
    "The formula is just what we described: `S = w_l * linguistic + w_s * structural + w_o * outcome`\n",
    "\n",
    "Your task:\n",
    "1. Implement `SalienceScorer` with configurable weights\n",
    "2. Weights must be updatable (constitutional rule: not hardcoded forever)\n",
    "3. `score` returns a `SalienceResult` with component breakdown\n",
    "4. `explain` returns plain English (constitutional rule: explain 0.7)\n",
    "5. State the **falsifiable claim**: what would make this scorer wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SalienceResult:\n",
    "    \"\"\"Result with component breakdown.\"\"\"\n",
    "    score: float\n",
    "    linguistic: float\n",
    "    structural: float\n",
    "    outcome: float\n",
    "    weights: dict\n",
    "\n",
    "    def explain(self) -> str:\n",
    "        \"\"\"Plain-English explanation.\"\"\"\n",
    "        # --- YOUR CODE BELOW ---\n",
    "        pass\n",
    "\n",
    "\n",
    "class SalienceScorer:\n",
    "    \"\"\"\n",
    "    Scores agent output for rule compliance using three signals.\n",
    "\n",
    "    Falsifiable claim: Rankings agree with expert intuitive ratings\n",
    "    (Spearman rho > 0.8). If not, recalibrate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w_linguistic=0.4, w_structural=0.4, w_outcome=0.2):\n",
    "        # --- YOUR CODE BELOW ---\n",
    "        pass\n",
    "\n",
    "    def update_weights(self, w_l: float, w_s: float, w_o: float) -> None:\n",
    "        \"\"\"Update weights. Must sum to 1.\"\"\"\n",
    "        # --- YOUR CODE BELOW ---\n",
    "        pass\n",
    "\n",
    "    def score(self, entry: BuildlogEntry) -> SalienceResult:\n",
    "        \"\"\"Score a single buildlog entry.\"\"\"\n",
    "        # --- YOUR CODE BELOW ---\n",
    "        pass\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ @dataclass\n",
    "# │ class SalienceResult:\n",
    "# │     score: float\n",
    "# │     linguistic: float\n",
    "# │     structural: float\n",
    "# │     outcome: float\n",
    "# │     weights: dict\n",
    "# │\n",
    "# │     def explain(self) -> str:\n",
    "# │         level = (\n",
    "# │             \"strong\" if self.score >= 0.75\n",
    "# │             else \"moderate\" if self.score >= 0.5\n",
    "# │             else \"weak\" if self.score >= 0.25\n",
    "# │             else \"minimal\"\n",
    "# │         )\n",
    "# │         parts = []\n",
    "# │         if self.linguistic >= 0.7:\n",
    "# │             parts.append(\"uses relevant vocabulary\")\n",
    "# │         elif self.linguistic >= 0.4:\n",
    "# │             parts.append(\"some vocabulary overlap\")\n",
    "# │         else:\n",
    "# │             parts.append(\"little vocabulary match\")\n",
    "# │         if self.structural >= 0.7:\n",
    "# │             parts.append(\"follows the prescribed pattern\")\n",
    "# │         elif self.structural >= 0.4:\n",
    "# │             parts.append(\"partially follows the pattern\")\n",
    "# │         else:\n",
    "# │             parts.append(\"doesn't follow the pattern\")\n",
    "# │         parts.append(\"task succeeded\" if self.outcome >= 0.5 else \"task failed\")\n",
    "# │         return f\"Score {self.score:.2f} -- {level} compliance. The output {', '.join(parts)}.\"\n",
    "# │\n",
    "# │ class SalienceScorer:\n",
    "# │     FALSIFIABLE_CLAIM = \"Rankings agree with expert ratings (Spearman rho > 0.8).\"\n",
    "# │\n",
    "# │     def __init__(self, w_linguistic=0.4, w_structural=0.4, w_outcome=0.2):\n",
    "# │         self.update_weights(w_linguistic, w_structural, w_outcome)\n",
    "# │\n",
    "# │     def update_weights(self, w_l, w_s, w_o):\n",
    "# │         total = w_l + w_s + w_o\n",
    "# │         assert abs(total - 1.0) < 1e-6, f\"Weights must sum to 1, got {total}\"\n",
    "# │         self.weights = {\"linguistic\": w_l, \"structural\": w_s, \"outcome\": w_o}\n",
    "# │\n",
    "# │     def score(self, entry):\n",
    "# │         l = linguistic_signal(entry.rule, entry.agent_output)\n",
    "# │         s = structural_signal(entry.rule, entry.agent_output)\n",
    "# │         o = outcome_signal(entry)\n",
    "# │         combined = self.weights[\"linguistic\"]*l + self.weights[\"structural\"]*s + self.weights[\"outcome\"]*o\n",
    "# │         return SalienceResult(round(combined, 4), round(l, 4), round(s, 4), round(o, 4), dict(self.weights))\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: scorer should be constructible and produce results\n",
    "scorer = SalienceScorer(w_linguistic=0.4, w_structural=0.4, w_outcome=0.2)\n",
    "r0 = scorer.score(ENTRIES[0])\n",
    "assert r0 is not None, \"scorer.score() returned None. Did you implement it?\"\n",
    "assert hasattr(r0, 'explain'), \"SalienceResult needs an explain() method.\"\n",
    "assert r0.score > 0.5, f\"Entry 0 (perfect compliance) should score > 0.5, got {r0.score}\"\n",
    "print(f\"Entry 0 score: {r0.score:.2f}\")\n",
    "print(f\"Explanation: {r0.explain()}\")\n",
    "\n",
    "# Tests pass. Moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a *salience scorer* — a weighted linear combination of three signals.\n",
    "Both of those are just names for what you already built: multiply each signal\n",
    "by its weight, sum the results.\n",
    "\n",
    "> **Go deeper**: You just built a weighted linear combination. For the probability\n",
    "> foundations underneath this: *Think Stats* Ch 1-2 (exploratory data analysis),\n",
    "> *Think Bayes* Ch 1 (computational Bayesian thinking), Blitzstein & Hwang Ch 1-2\n",
    "> (formal probability framework).\n",
    "\n",
    "Now let's see if it actually works across all 10 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Problem 6: Run the Scorer, Visualize, Test the Falsifiable Claim\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "Now run the `scorer` from Problem 5 against all `ENTRIES` from Problem 1.\n",
    "\n",
    "Your task:\n",
    "1. Score all 10 entries\n",
    "2. Create a side-by-side bar chart: salience score vs. intuitive rating\n",
    "3. Compute Spearman rank correlation — does it beat 0.8?\n",
    "4. Print the explanation for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE BELOW ---\n",
    "# Score all entries, plot comparison, compute Spearman correlation\n",
    "\n",
    "\n",
    "# >>> SOLUTION (collapsed by default)\n",
    "# ┌─────────────────────────────────────────────────────────────────────────────\n",
    "# │ results = [scorer.score(e) for e in ENTRIES]\n",
    "# │ salience_scores = [r.score for r in results]\n",
    "# │ intuitive = [e.intuitive_compliance for e in ENTRIES]\n",
    "# │\n",
    "# │ rho, pval = scipy_stats.spearmanr(salience_scores, intuitive)\n",
    "# │ print(f\"Spearman rho: {rho:.3f}  (p={pval:.4f})\")\n",
    "# │ print(f\"Falsifiable claim threshold: rho > 0.8\")\n",
    "# │ print(f\"Result: {'PASS' if rho > 0.8 else 'NEEDS RECALIBRATION'}\")\n",
    "# │ print()\n",
    "# │\n",
    "# │ x = np.arange(len(ENTRIES))\n",
    "# │ width = 0.35\n",
    "# │ fig, ax = plt.subplots(figsize=(14, 5))\n",
    "# │ bars1 = ax.bar(x - width/2, salience_scores, width, label='Salience Score', color='#2196F3')\n",
    "# │ bars2 = ax.bar(x + width/2, intuitive, width, label='Intuitive Rating', color='#FF9800')\n",
    "# │ # Mark failed tasks\n",
    "# │ for i, e in enumerate(ENTRIES):\n",
    "# │     if not e.task_succeeded:\n",
    "# │         ax.annotate('FAILED', (i, max(salience_scores[i], intuitive[i]) + 0.05),\n",
    "# │                     ha='center', fontsize=8, color='red')\n",
    "# │ ax.set_xlabel('Entry')\n",
    "# │ ax.set_ylabel('Score')\n",
    "# │ ax.set_title('SalienceScorer vs. Intuitive Ratings')\n",
    "# │ ax.set_xticks(x)\n",
    "# │ ax.legend()\n",
    "# │ ax.set_ylim(0, 1.2)\n",
    "# │ plt.tight_layout()\n",
    "# │ plt.show()\n",
    "# │\n",
    "# │ print(\"\\nExplanations:\")\n",
    "# │ for i, (e, r) in enumerate(zip(ENTRIES, results)):\n",
    "# │     status = \"OK\" if e.task_succeeded else \"FAILED\"\n",
    "# │     print(f\"\\n  Entry {i} [{status}]: {e.task[:50]}\")\n",
    "# │     print(f\"    {r.explain()}\")\n",
    "# │     print(f\"    Components: L={r.linguistic:.2f} S={r.structural:.2f} O={r.outcome:.2f}\")\n",
    "# └─────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Backfill: Why a Weighted Linear Combination?\n",
    "\n",
    "We made two assumptions that are both wrong and both useful:\n",
    "\n",
    "1. **Linearity**: Signals combine additively. In reality, high structural + high\n",
    "   linguistic is stronger evidence than either alone (interaction effects).\n",
    "   But linear is the simplest baseline.\n",
    "\n",
    "2. **Independence**: The three signals are independent. They're not — vocabulary\n",
    "   overlap correlates with structural compliance. But treating them as independent\n",
    "   lets us build and test each detector separately.\n",
    "\n",
    "Both assumptions break down. That's fine. Module 0.2 builds the probability\n",
    "foundations to handle more sophisticated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# EXERCISES\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## Exercise 1: Break the Scorer\n",
    "\n",
    "Add 3 new `BuildlogEntry` instances to `ENTRIES` that expose failure modes of\n",
    "the `SalienceScorer`. For each entry, explain:\n",
    "- What the scorer gets wrong (predicted score vs. your intuitive rating)\n",
    "- Why it fails (which signal is misleading)\n",
    "- How you'd fix it (what signal or logic would handle this case)\n",
    "\n",
    "**Success criteria**: At least one entry where the scorer's ranking disagrees\n",
    "with intuition by > 0.3.\n",
    "\n",
    "## Exercise 2: Weight Tuning\n",
    "\n",
    "Try different weight configurations on the 10 entries:\n",
    "- `(0.7, 0.2, 0.1)` — linguistic-heavy\n",
    "- `(0.2, 0.7, 0.1)` — structural-heavy\n",
    "- `(0.33, 0.33, 0.34)` — uniform\n",
    "\n",
    "**Success criteria**: Report the Spearman rho for each configuration. Identify\n",
    "which gives the best correlation. If any configuration achieves rho > 0.9,\n",
    "explain whether that's likely overfitting to 10 data points (hint: it probably is).\n",
    "\n",
    "## Exercise 3 [PUBLISH]: Write the \"Contains Check Takedown\"\n",
    "\n",
    "Write 500-800 words explaining why substring matching fails for evaluating\n",
    "agent rule compliance. Target: practitioners building AI agent systems.\n",
    "\n",
    "Structure:\n",
    "1. The problem: you have rules, you need to evaluate compliance\n",
    "2. The naive approach: `contains` / substring matching\n",
    "3. Three failure modes (use examples from this notebook)\n",
    "4. The alternative: decompose into linguistic + structural + outcome signals\n",
    "5. Why this matters for bandit-based rule learning systems\n",
    "\n",
    "Draft for: *\"Here's what everyone gets wrong about evaluating AI agent output\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 workspace\n",
    "draft = \"\"\"\n",
    "# Here's What Everyone Gets Wrong About Evaluating AI Agent Output\n",
    "\n",
    "TODO: Write your draft here.\n",
    "\"\"\"\n",
    "print(draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# OUTRO\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "## What Just Happened\n",
    "\n",
    "You started with one broken evaluation — an agent punished for following a rule —\n",
    "and built a scorer that actually works. Here's the path you took:\n",
    "\n",
    "1. **Felt the problem** on a single example (`contains` returned 0.0 on perfect compliance)\n",
    "2. **Built vocabulary matching** to catch what `contains` missed\n",
    "3. **Expanded to a full test suite** to find edge cases\n",
    "4. **Added structural checks** to catch false positives (Entry 3)\n",
    "5. **Added outcome signal** to catch broken implementations (Entries 7-8)\n",
    "6. **Packaged it** as a `SalienceScorer` with configurable weights\n",
    "7. **Validated** against your intuitive ratings using Spearman correlation\n",
    "\n",
    "The formula — `S = w_l * linguistic + w_s * structural + w_o * outcome` — is a\n",
    "weighted linear combination. It's the simplest model that could work.\n",
    "\n",
    "You also established three constitutional rules for the entire arc:\n",
    "\n",
    "1. Every scoring function must have a falsifiable claim\n",
    "2. Weights must be updatable from data, not hardcoded forever\n",
    "3. If you can't explain what a score of 0.7 means in plain English, the scorer isn't ready\n",
    "\n",
    "## Publication Note\n",
    "\n",
    "Exercise 3 is a draft for *\"Here's what everyone gets wrong about evaluating\n",
    "AI agent output.\"* Run an edit pass and it's ready to publish.\n",
    "\n",
    "## What's Next\n",
    "\n",
    "The weights are hand-picked. Why 0.4/0.4/0.2? Because it felt right. That's\n",
    "not science. In Module 0.2, we build the probability foundations to make this\n",
    "rigorous — so the weights can be learned from data instead of vibes.\n",
    "\n",
    "--> [Module 0.2: Probability & Counting](../module-0.2-probability-counting/0.2-probability-counting-core.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# RESOURCES\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "- **Salience (cognitive science)**: Salience as attention-weighted relevance comes\n",
    "  from cognitive/perceptual psychology. Our scorer approximates what a human\n",
    "  reviewer does intuitively.\n",
    "- **Weighted linear combinations**: Any introductory linear algebra text covers\n",
    "  why this is the simplest useful model. We'll formalize this in Arc 1.\n",
    "- **From the archive**: `archive/v1-week-based/notebooks/` — earlier explorations\n",
    "  of the bandit that this module's scorer feeds into.\n",
    "\n",
    "### Companion Texts\n",
    "\n",
    "- **Downey, *Think Stats* (2nd ed.)**: Ch 1-2 — exploratory data analysis foundations\n",
    "- **Downey, *Think Bayes* (2nd ed.)**: Ch 1 — computational approach to Bayesian thinking\n",
    "- **Blitzstein & Hwang, *Introduction to Probability***: Ch 1-2 — formal probability\n",
    "  framework we'll use starting in Module 0.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
